[{"id":"02d29d69da92d1495f7e9b3c258ba0cc","title":"一致性Hash","content":"背景跨一组节点分发数据的行为称为数据分区。当我们尝试分发数据时，有两个挑战：\n\n我们如何知道特定数据将存储在哪个节点上？\n当我们添加或删除节点时，我们如何知道哪些数据将从现有节点移动到新节点？此外，当节点加入或离开时，我们如何最大限度地减少数据移动？\n\n一种简单的方法将使用适当的hash函数，将数据键映射到一个数字。然后，通过对此数量和服务器总数求模来找到服务器。例如：\n\n\n上图中描述的方案解决了寻找用于存储&#x2F;检索数据的服务器的问题。但当我们添加或删除服务器时，我们必须重新映射所有键，并根据新的服务器计数移动数据，这将是一个完全混乱的过程！\n定义使用一致的hash算法跨节点分发数据。一致的hash将数据映射到物理节点，并确保在添加或删除服务器时只移动一小部分key。\n解决一致hash技术将分布式系统管理的数据存储在环中。环中的每个节点都被分配了一系列数据。以下是一致hash环的一个示例：\n\n\n使用一致性Hash，可以将环划分为较小的预定义范围。每个节点都被分配到这些范围中的一个。范围的开始称为令牌。这意味着将为每个节点分配一个令牌。分配给每个节点的范围计算如下：\n范围开始：一个令牌值范围结束：下一个令牌值-1\n以下是上图中描述的四个节点的令牌和数据范围\n\n\n\nServer\nToken\nRange Start\nRange End\n\n\n\nServer 1\n1\n1\n25\n\n\nServer 2\n26\n26\n50\n\n\nServer 3\n51\n51\n75\n\n\nServer 4\n76\n76\n100\n\n\n每当系统需要读取或写入数据时，它执行的第一步是将MD5Hash算法应用于Key。该Hash算法的输出确定数据位于哪个范围内，从而确定数据将存储在哪个节点上。正如我们在上面看到的，每个节点都应该存储固定范围的数据。因此，从Key生成的Hash告诉我们将在其中存储数据的节点。\n\n\n当向环中添加节点或从环中移除节点时，如上所述的一致性Hash方案工作得很好，因为只有下一个节点受到影响。例如，当一个节点被移除时，下一个节点将负责存储在移除节点上的所有Key。然而，这种方案可能会导致数据和负载分布不均匀。这个问题可以通过虚拟节点来解决。\n虚拟节点在任何分布式系统中添加和删除节点都很常见。现有节点可能会死亡，可能需要停用。同样，可以将新节点添加到现有群集中以满足不断增长的需求。为了有效地处理这些场景，一致hash使用虚拟节点(或虚拟节点)。\n正如我们在上面看到的，基本一致Hash算法为每个物理节点分配单个令牌(或连续的Hash范围)。这是一种静态范围划分，需要根据给定的节点数量计算令牌。此方案使添加或替换节点成为一项代价高昂的操作，因为在本例中，我们希望重新平衡数据并将其分发到所有其他节点，从而导致移动大量数据。以下是与手动和固定范围划分相关的一些潜在问题：\n\n添加或删除节点：添加或删除节点将导致重新计算令牌，从而为大型集群带来巨大的管理开销。\n热点：由于每个节点都被分配了一个大范围，如果数据分布不均匀，一些节点可能会成为热点。\n节点重建：由于每个节点的数据可能被复制到固定数量的其他节点上(以实现容错)，因此当我们需要重建节点时，只有其副本节点可以提供数据。这给副本节点带来了很大的压力，并可能导致服务降级。\n\n为了处理这些问题，一致性Hash引入了一种将令牌分发到物理节点的新方案。不是将单个令牌分配给节点，而是将哈希范围划分为多个较小的范围，并为每个物理节点分配多个这些较小的范围。这些子范围中的每一个都被视为Vnode。对于VNodes，一个节点不是只负责一个令牌，而是负责许多令牌(或子范围)。\n\n\n实际上，虚拟节点在整个集群中随机分布，并且通常不连续，因此不会将两个相邻的虚拟节点分配给相同的物理节点或机架。此外，节点确实携带其他节点的副本以实现容错。此外，由于群集中可能存在异类计算机，因此某些服务器可能会比其他服务器包含更多的Vnode。下图显示了物理节点A、B、C、D和E如何使用一致哈希环的V节点。每个物理节点都被分配了一组Vnode，并且每个Vnode被复制一次。\n\n\n虚拟节点的优势\n由于虚拟节点通过将哈希范围划分为更小的子范围来帮助在群集上的物理节点上更均匀地分配负载，因此在添加或删除节点后加快了重新平衡过程。当添加新节点时，它会从现有节点接收许多Vnode，以维护平衡的集群。同样，当一个节点需要重建时，许多节点都会参与重建过程，而不是从固定数量的副本获取数据。\n虚拟节点使维护包含异类机器的集群变得更容易。这意味着，使用VNodes，我们可以将大量的子范围分配给功能强大的服务器，将较少数量的子范围分配给功能较弱的服务器。\n与一个大范围相比，由于V节点帮助为每个物理节点分配较小的范围，这降低了出现热点的可能性。\n\n例子Dynamo和Cassandra使用一致性Hash来跨节点分发数据。\n","slug":"一致性Hash","date":"2022-06-19T03:15:56.000Z","categories_index":"","tags_index":"系统设计模式","author_index":"李志博的博客"},{"id":"1d4e4cde9c655456c3b6951c0ffa8502","title":"BloomFilters","content":"背景如果我们在一组数据文件中存储了大量结构化数据(由记录ID标识)，那么知道哪个文件可能包含我们需要的数据的最有效方法是什么？我们不想读取每个文件，因为这样会很慢，而且我们必须从磁盘读取大量数据。一种解决方案是在每个数据文件上建立索引，并将其存储在单独的索引文件中。每个索引文件将根据记录ID进行排序。现在，如果我们想要在此索引中搜索ID，最好的方法是二进制搜索。我们还能做得更好吗？\n定义使用Bloom Filters快速查找某个元素是否可能出现在集合中。\n解决方案Bloom Filter数据结构告诉我们元素是否可能在集合中，或者肯定不在集合中。唯一可能的错误是误报，即搜索不存在的元素可能会给出错误的答案。过滤器中的元素越多，错误率就越高。空的布隆过滤器是由m个比特组成的比特数组，全部设置为0。还有k个不同的hash函数，每个hash函数将集合元素哈希到m个比特位置之一。\n\n要添加一个元素，将其提供给哈希函数以获得k个位位置，并将这些位置处的位设置为1。\n要测试元素是否在集合中，请将其提供给哈希函数以获得k个位位置。\n如果这些位置的任一位为0，则该元素肯定不在该集合中。\n如果全部为1，则元素可能在集合中。\n\n\n\n下面是一个包含三个元素P、Q和R的Bloom过滤器。它由20位组成，使用三个散列函数。彩色箭头指向集合的元素映射到的位。\n\n\n\n元素X肯定不在集合中，因为它散列到包含0的位位置。\n对于固定的错误率，添加新元素和测试成员资格都是恒定的时间操作，而具有‘n’个元素空间的过滤器需要O(N)O(N)空间。\n\n示例：Bigtable在Bigtable(和Cassandra)中，任何读取操作都必须从 Tablet组成的SSTables中读取。如果这些SSTable不在内存中，读操作可能会发起多次磁盘访问。为了减少磁盘访问次数，Bigtable使用了Bloom Filters。\nBloom Filters是为SSTables(特别是为局部性组)创建的。它们通过预测SSTable是否可能包含对应于特定行或列对的数据来帮助减少磁盘访问次数。对于某些应用程序，用于存储Bloom过滤器的少量Tablet服务器内存可显著减少磁盘寻道次数，从而提高读取性能。\n","slug":"BloomFilters","date":"2022-06-19T01:28:08.000Z","categories_index":"","tags_index":"系统设计模式","author_index":"李志博的博客"},{"id":"82b321e71a818dfcf2a53be9140d17a7","title":"重新设计数据索引结构,缓存内存从2.3G降到了59MB","content":"背景之前写过的文件缓存的设计，后来我发现还是有很多缺点的，比如说\n\n实际索引结构内存占用也不小，而且只有hash索引和entity列索引，那么其他搜索场景，可能依然点需要通过顺序遍历的方式\n另外搜索命中entity列索引的ID列表，有可能会分散在各个ID和offset映射的区间，为了读到数据，最终还是要把所有文件区间都读到内存，失去了内存优化的意义，并且性能会很慢，平时IO读一次要100多ms，但是有时候偶尔会有一次1秒以上的情况（单次读）。\n\n效果介绍于是我设计了新的索引结构，新的结构支持任何搜索条件都可以命中索引，并且不在需要文件IO，纯内存存储，内存占用特别小（原始数据不算索引,大小是2.3G,结构是List&lt;Map&lt;String,Object&gt;&gt; ，新的索引占用内存大小是59MB）,以下是新老内存占用效果对比。\n\n\n以上是纯原始数据要占用2.3G，MyList 是我临时写的1个数据包装类，目的是为了方便在Jprofiler看内存大小。以下是MyList类的样子。\n@Data\n  private static class MyList &#123;\n      private final List&lt;Map&lt;String, Object&gt;&gt; maps;\n  &#125;\n\n下面是新的索引结构占用内存效果，所有索引以及索引相关的一些元数据都存储在CubeDataCacheMetaData 这个类里，可以看到它最终占用内存大小是59MB\n\n\n新索引的设计\n\n以上的图是新索引的图形化表示，每个列映射列下去重后的值（因为存的都是维度值，所以去重后不会有太多），每个值映射的是拥有这个数据的ID范围，这里为了节省内存，只存1个范围，也就是2个数字，起始值和结束值。为了便于理解，可以把上图所表示的索引结构想象成下面这样，实际为了进一步优化，会有稍微的不同:\nMap&lt;String,Map&lt;String,List&lt;Range&lt;Integer&gt;&gt;&gt;&gt;\n\nRange类是Guava的1个类，表示1个范围，里面可以存储lowerBound(起始值)和upperBound(结束值)2个范围，并且它自带的intersection方法求交集也正是我们所需要的。\n那么有了这个索引结构，假设说，我要搜索的2个条件是列1 &#x3D; 去重后的值1，列2等于去重后的值B，因为是map结构，所以我可以很快速的找到2个条件所表示的ID范围\n\n\n\n列1 （去重后的值1）\n列2（去重后的值B）\n\n\n\n1-120\n1-49\n\n\n178-190\n81-100\n\n\n有了这几个ID范围，那么列1的范围和列2的范围最终我去做1个交集，其实就是最终我们要搜索的ID集合结果了，这个例子的结果就是2个ID范围，1-49和81-100。\n以上算不同条件的ID的交集，有很多提高搜索速度方便的优化点 ：\n\n在搜索之前，我会按每个搜索条件得到的ID范围再次进行排序\n然后对按每个列得到的搜索范围列表的大小进行排序，小的范围在前，类似于驱动表的意思。\n接下来，我遍历排序后第一个条件的范围列表，拿到每一个范围，跟后续条件的范围列表不断比较去算他们的交集，由于范围之前是有序的，在算交集之前和过程中，还会做如下处理\n如果2个范围翻墙不相交，直接return\n如果第一个范围在后面条件的范围集合的第5个范围里发现是存在交集的，那么算第2个范围时，可以直接从第5个范围的基础上开始算，不用每次从头开始，因为范围是有序的。\n\n\n\n通过ID算交集这种数字比较的方式，去替代以前不命中索引，或者只命中1个entity索引后，其他列扔需要循环顺序字符串equals比较的方式，极大的提高了搜索速度。\n现在得到了ID，但是我们最终要向上返回的仍然是List&lt;Map&lt;String,Object&gt;&gt;的结构，之前说不在需要文件IO的方式，是因为我们可以通过ID到前面图里说的索引结构里反向查出来ID对应的每个列的值是多少，从而现去组装出我们的数据。\n在这里，也有2个优化：\n\n如果搜索条件就1个值，那压根不需要去索引结构里查，既然命中了这个ID，那么值一定是这个。\n由于每个值映射的ID范围都是有序的，所以可以通过二分搜索的方式，实现快速定位。\n\n最后还有1点要说的是，其实真实的存储结构，并不是前面说的\nMap&lt;String,Map&lt;String,List&lt;Range&lt;Integer&gt;&gt;&gt;&gt;\n\n而是\nMap&lt;String,Map&lt;String,LongArrayList&gt;&gt;\n\nLongArrayList 是eclipse-collection 的类，看名字就可以知道，它是1个存储long类型的List，并且它存储的是基本类型的long 不存在装箱的问题。\n而用long去替代Range主要是考虑1个long是64位恰好可以利用低32位去存1个int,高32位去存1个，因为整个公司所有项目，最大的事实表也就6000多万，所以int类型表示其中1个ID绝对是足够的，这样用long去替代Range，也就避免了装箱和Java引用类型对象算占用的内存空间浪费。\nHash索引的内存优化在老版本里Hash索引的结构大概如下\nIntObjectHashMap&lt;IntArrayList&gt;\n\n以上也是eclipse-collections里的类，为了便于理解，也可以想象成\nMap&lt;Integer,List&lt;Integer&gt;&gt;\n\n之所以这存是因为虽然绝大多数Map里的Key也就是Hash值和ID也就是Value的关系是一对一映射的，但是Hash是存在冲突的，所以我们用了1个List来存储。\n但是上述结构之前设计不好的地方是，毕竟绝大多数的数据其实是Hash不冲突的，但是为此却额外创建了1个List,导致了内存的浪费。\n新的结构我在原有的结构基础上增加了1个\nIntIntHashMap\n\n我们可以理解成:\nMap&lt;Integer,Integer&gt;\n\n这样默认Hash的索引存在这里，如果真的发生了Hash冲突，在升级到原有的结构里，避免创建额外的List浪费内存。\n值列表存储的优化之前事实表里的关于值的列，其实有2列大概如下\n\n\n\ndecimal_val\nstring_val\n\n\n\n190000\nnull\n\n\n-52932.410000\nnull\n\n\n…\n…\n\n\n一般来讲，绝大多数Case用的其实都是decimal_val列，少数可能会只用string_val 列，或者string_val 和 decimal_val 混用的情况。\n之前版本我的存储结构大概如下:\nList&lt;CubeDataValue&gt; list;\n\nCubeDataValue 里有2个属性1个decimalValue,1个stringValue ，这样来实现以上3种可能的不同使用情况。\n但是上面的这种设计，对于绝大多数只使用decimal_val的场景来说，就是一种浪费，所以新的存储方案，我用1个思维导图来表示如下:\n\n\n通过这样1个对象，3种不同的实现，来兼容3种不同的情况，对于绝大多数场景来讲，就是纯Double类型的实现，里面也就是1个DoubleArrayList里存的是基本类型的double值\n纯String类型的实现比较简单，就是普通的List&lt;String&gt;, 如果是混合类型实现，我将数据也是分double和string 2块分开来存的，这么做的原因也是为了避免装箱浪费内存。但是这么做以后，当数据要进行查找的时候，就没办法通过ID定位具体的数据，所以就有了后面2个结构。\n先说下LongArrayList ，这个存储的long是用来记录位操作，如果是String的值，那么在对应id - 1 的位上，设置1，否则就是0，那么当根据ID来查找时候，就可以知道对应的数据是1个字符串还是1个数字，从而知道去哪查。像我测试的这个Case来讲，1个long可以记录64条数据，117w的数据用18000多个long就可以存下了。\n但是光知道去哪查还不够，还需要知道查找的index,所以还需要1个IntIntHashMap存储的就是ID和实际集合Index的映射关系。\n以上的思路，适用于虽然是混合类型，但是数字类型的值占绝大多数的情况，如果是字符串占绝大多数的话，感觉反而不太好，算是这次设计不太好的地方。\n总结经过这次的优化，我觉得要想内存占用的少，以后可以多往如下几个点去思考:\n\n优化存储结构，尽量避免数据在内存中发生重复\n能存范围的数据，不要存真实的值\n尽可能避免基本类型的装箱，尽量用基本类型去存储。\n多思考位操作，比如说用1个long存储2个Int 这种方式来减少数据的存储。\n\n","slug":"重新设计数据索引结构-缓存内存从2-3G降到了59MB","date":"2022-06-18T03:22:34.000Z","categories_index":"","tags_index":"Java,工作","author_index":"李志博的博客"},{"id":"2e752d116822fcd8f3fa0fbea2092f6c","title":"BigDecimal类的四个常见陷阱以及如何避免它们","content":"在Java中进行货币计算时，您可以使用java.math.BigDecimal–但要注意该类的一些独特挑战。\n在使用Java进行商业计算时，尤其是对于货币，最好使用该类来避免与浮点算术相关的问题，如果使用两个基元类型之一：Float或Double(或它们的盒装类型对应类型)，则可能会遇到这些问题。\n事实上，该类包含许多方法，可以满足大多数常见业务计算的要求。\n然而，我想提请大家注意BigDecimal类的四个常见陷阱。当您使用常规的BigDecimal API和使用一个新的、可扩展的定制类时，可以避免它们。\n那么，让我们从常规的 BigDecimal API 开始。\n陷阱1: 双重构造函数考虑下面的例子:\nBigDecimal x &#x3D; new BigDecimal(0.1);\nSystem.out.println(&quot;x&#x3D;&quot; + x);\n\n下面是控制台的输出:\nx&#x3D;0.1000000000000000055511151231257827021181583404541015625\n\n如您所见，此构造函数的结果可能有些不可预测。这是因为浮点数在计算机硬件中表示为基数2(二进制)小数。但是，大多数小数不能完全表示为二进制小数。因此，机器中实际存储的二进制浮点数仅接近您输入的十进制浮点数。因此，传递给Double构造函数的值不完全等于0.1。\n相反，字符串构造函数是完全可预测的，并且生成的值恰好等于0.1，不出所料。\nBigDecimal y &#x3D; new BigDecimal(&quot;0.1&quot;);\nSystem.out.println(&quot;y&#x3D;&quot; + y);\n\n现在，控制台输出如下：\ny&#x3D;0.1\n\n因此，您应该优先使用字符串构造函数，而不是Double构造函数。\n陷阱2: 静态ValueOf方法如果使用静态BigDecimal.valueOf(Double)方法创建BigDecimal，请注意其精度有限。\nBigDecimal x &#x3D; BigDecimal.valueOf(1.01234567890123456789);\nBigDecimal y &#x3D; new BigDecimal(&quot;1.01234567890123456789&quot;);\nSystem.out.println(&quot;x&#x3D;&quot; + x);\nSystem.out.println(&quot;y&#x3D;&quot; + y);\n\n上面的代码生成以下控制台输出：\nx&#x3D;1.0123456789012346\ny&#x3D;1.01234567890123456789\n\n在这里，该值丢失了四位小数位，因为Double的精度只有15-17位(Float的精度只有6-9位)，而BigDecimal的精度是任意的(仅受内存限制)。\n因此，使用字符串构造函数实际上是一个好主意，因为有效地避免了由双重构造函数引起的两个主要问题。\n陷阱#3：Equals(BigDecimal)方法BigDecimal x &#x3D; new BigDecimal(&quot;1&quot;);\nBigDecimal y &#x3D; new BigDecimal(&quot;1.0&quot;);\nSystem.out.println(x.equals(y));\n\n控制台输出如下：\nFalse\n\n此输出是由于BigDecimal由任意精度的未缩放整数值和32位整数小数位数组成，这两个值都必须等于要比较的另一个BigDecimal的相应值。在这种情况下\n\nX的未缩放值为1，比例为0。\nY的未缩放值为10，比例为1。\n\n因此，X不等于Y。\n因此，不应该使用equals()方法比较BigDecimal的两个实例，而应该使用CompareTo()方法，因为它比较了BigDecimal的两个实例表示的数值(x&#x3D;1；y&#x3D;1.0)。下面是一个例子：\nSystem.out.println(x.compareTo(y) &#x3D;&#x3D; 0);\n\n现在，控制台输出如下：\nTrue\n\n陷阱#4：round(MathContext)方法一些开发人员可能会倾向于使用round(new MathContext(precision, roundingMode))方法将BigDecimal舍入为(比方说)两位小数。这不是个好主意。\nBigDecimal x &#x3D; new BigDecimal(&quot;12345.6789&quot;);\nx &#x3D; x.round(new MathContext(2, RoundingMode.HALF_UP));\nSystem.out.println(&quot;x&#x3D;&quot; + x.toPlainString());\nSystem.out.println(&quot;scale&#x3D;&quot; + x.scale());\n\n上面的代码生成以下控制台输出，因此x不是12345.68的预期值，刻度也不是2的预期值：\nx&#x3D;12000 \nscale&#x3D;-3\n\n该方法不会对小数部分进行舍入，但会将未按比例调整的值舍入到给定的有效位数(从左到右计数)，小数点保持不变，在上面的示例中，小数点的小数位数为-3。\n那么，这里发生了什么？\n未缩放值(123456789)被四舍五入为两位有效数字(12)，这表示精度为2。但是，由于小数点保持不变，因此此BigDecimal表示的实际值为12000.0000。这也可以写为12000，因为小数点右侧的四个零没有意义。\n但规模又如何呢？为什么它是-3而不是0，正如您对值12000的预期？\n这是因为这个BigDecimal的无标度值是12，因此，它必须乘以1000，这是10的3次方，(12x103)等于12000。\n因此，正小数位数表示小数位数(即小数点右侧的位数)，而负数位数表示小数点左侧的无用位数(在本例中是尾随的零，因为它们只是指示数字小数位数的占位符)。\n最后，由BigDecimal表示的数字因此是unscaledValue x 10刻度。\n还要注意，上面的代码使用了toPlainString()方法，该方法不以科学记数法(1.2E+4)显示结果。\n若要获得12345.68的预期结果，请尝试使用setScale(Scale，oundingMode)方法，例如：\nBigDecimal x &#x3D; new BigDecimal(&quot;12345.6789&quot;);\nx &#x3D; x.setScale(2, RoundingMode.HALF_UP);\nSystem.out.println(&quot;x&#x3D;&quot; + x));\n\n现在控制台输出的就是预期的结果。\n现在，控制台输出是预期的：\nx&#x3D;12345.68\n\n方法的作用是：根据指定的舍入模式将小数部分舍入到小数点后两位。\n顺便说一句，您可以使用ROUND(new MathContext(Precision，oundingMode))方法进行传统的舍入。但这需要您知道计算结果小数点左侧的总位数。请考虑以下示例：\nBigDecimal a &#x3D; new BigDecimal(&quot;12345.12345&quot;);\nBigDecimal b &#x3D; new BigDecimal(&quot;23456.23456&quot;);\nBigDecimal c &#x3D; a.multiply(b);\nSystem.out.println(&quot;c&#x3D;&quot; + c);\n\n现在，控制台输出如下：\nc&#x3D;289570111.3153564320\n\n要将c舍入到小数点后两位，您必须使用精度为11的MathContext对象，例如：\nBigDecimal d &#x3D; c.round(new MathContext(11, RoundingMode.HALF_UP));\nSystem.out.println(&quot;d&#x3D;&quot; + d);\n\n上面的代码生成以下控制台输出：\nd&#x3D;289570111.32\n\n小数点左侧的总位数可按如下方式计算：\nbigDecimal.precision() - bigDecimal.scale() + newScale\n\n\nbigDecimal.precision() 是未四舍五入的总长度。\nbigDecimal.scale() 是未四舍五入小数的长度。\nnewScale 是您指定的新的小数部分长度。\n\n所以，下面代码:\nBigDecimal e &#x3D; c.round(new MathContext(c.precision() - c.scale() + 2, RoundingMode.HALF_UP));\n\n控制台输出\ne&#x3D;289570111.32\n\n然而，如果你将这个表达式\nc.round(new MathContext(c.precision() - c.scale() + 2, RoundingMode.HALF_UP));\n\n设置为以下表达式\nc.setScale(2, RoundingMode.HALF_UP);\n\n很明显，您会选择哪一个来确保代码的可读性和简明性。\n","slug":"BigDecimal类的四个常见陷阱以及如何避免它们","date":"2022-06-11T02:14:48.000Z","categories_index":"","tags_index":"Java","author_index":"李志博的博客"},{"id":"725e856f26b6befd144f449656c3dc6b","title":"SpringBoot单测通过注解覆盖Spring配置","content":"在我们写SpringBoot的单测的时候,有时候可能需要指定特殊的配置,用来覆盖原有的配置，以前的做法通常是2种\n\n在IDEA的RUN&#x2F;DEBUG Configurations 弹窗上设置-D启动参数\n在单测上人工写1个static 代码块，然后里面写System.setProperty\n\n以上第一种方式，如果我点的是IDEA左侧的启动按钮，会发现自动创建1个新的RUN&#x2F;DEBUG 配置,以前设置的配置就不起左右了， 还需要人工修改太麻烦。\n第二种方式又感觉不够优雅。\n昨天我在写单测的时候，又网上搜了下，发现原来SpringBoot本身为我们提供了1个叫@TestPropertySource 的注解,具体使用实例如下\n@SpringBootTest\n@TestPropertySource(properties &#x3D; &#123;\n        &quot;cube.cache.cacheFileDir&#x3D;&#x2F;tmp&#x2F;memory-financial-model-server&quot;,\n        &quot;seepln.datasource.select&#x3D;default&quot;,\n        &quot;datasource.mysql.password.encrypt&#x3D;false&quot;,\n        &quot;spring.datasource.hikari.jdbc-url&#x3D;jdbc:mysql:&#x2F;&#x2F;localhost:3306&#x2F;default_fxfwtc?autoReconnect&#x3D;true&amp;useUnicode&#x3D;true&amp;allowMultiQueries&#x3D;true&amp;characterEncoding&#x3D;utf8&quot;,\n        &quot;spring.datasource.hikari.username&#x3D;root&quot;,\n        &quot;spring.datasource.hikari.password&#x3D;xxx&quot;\n&#125;)\n@ActiveProfiles(&quot;test&quot;)\nclass CubeDataCacheServiceTest &#123;\n&#125;\n\n只要像上面那样声明在测试类的上面,并按照这个格式去书写配置的KV项就好了。\n","slug":"SpringBoot单测通过注解覆盖Spring配置","date":"2022-06-05T06:58:09.000Z","categories_index":"","tags_index":"SpringBoot,Spring","author_index":"李志博的博客"},{"id":"2d0169f6f2111380cc34e7095654794f","title":"文件缓存的设计","content":"背景最近工作中需要做1个很有意思并且有一定挑战的事情，项目里有1个接口，如果是第一次查询，会一次性查整个表的所有数据，然后缓存在JVM内部，后续的搜索就都会查这个缓存，这个方案肯定是不合理的，但是未来也是有计划迁移到另外1个替代它的项目上，但是在迁移之前，领导叫我临时性的做下内存优化，避免在迁移之前系统因内存不足发生崩溃或者频繁Full GC之类的问题。\n整体实现思路首先粗粒度的说下大体实现思路, 为了描述清楚，我直接用表格的方式去对比新老实现之前的区别\n\n\n\n老实现\n新实现\n\n\n\n初始化缓存数据是直接select 全表到Caffeine里\n是通过游标的方式不断读取部分数据，追加写入到本地文件\n\n\nCaffeine里存储整个表的全量数据以及索引数据\nCaffine里只存储索引数据以及表里部分热数据。\n\n\nhash 索引 + entity索引\n在老实现基础上，增加1个ID索引并映射要修改的值和文件的Offset\n\n\n更新缓存和更新DB是分开的，会存在一致性问题\n通过读写锁的方式，对更新DB和更新缓存套1个写锁，缓存内部搜索加读锁，避免一致性问题。\n\n\n通过以上表格对比，这里在说明下新老实现的思路\n\n老实现是一次性select全表到内存，并维护了2个额外的索引，1个是Hash索引，1个是Entity索引，由于全量数据和索引都在内存里，所以内存压力较大。\n新实现是利用局部数据的思想，以游标的方式读表里数据，最终内存里缓存的是全量的索引数据，但是不涉及表的原始数据，所以比较节省内存。\n\n实现细节初始化缓存新实现在缓存的设计上分了2个独立的Caffine对象，用于缓存不同类型的数据，1个是缓存的元数据对象，另外1个是缓存的热数据，我们这里先来看在初始化缓存元数据的时候都做了哪些细节上的设计。\n在缓存对外暴露的几个接口里，无论是读还是写，在第一行，都会通过caffeine去获取缓存元数据对象，如果取不到就会触发回调加载的逻辑，而在回调的里面就是之前说的，通过游标的方式去分页加载数据。\n为什么用游标的方式是因为我们的MYSQL的事实表是没有ID的，主键是所有维度列组合在一起的联合索引（数量一般在七八个左右），所以我没办法按以前优化分页的逻辑用ID去卡，也没有时间列，更不可能去写普通的分页查询（那样会有深分页性能问题），所以这里选择了游标的方式去读数据。整个初始化缓存的流程大概如下图所示：\n\n\n读完数据首先要做的事是为每一条数据有序生成1个ID，这样做的目的是为了后续搜索的时候，实现顺序读，减少磁盘IO次数。\n接下来就是要写磁盘，然后更新内存的索引，那么首先在写磁盘的时候，就不能一条1条数据循环调用文件的API去写磁盘，那样性能太差，假设如果有100万数据，相当于我要调用100万次write方法去写入，这块的性能我也简单做了下测试，比如说一次写1w条数据跟一次写1条，其实花费的时间并不是太大，可是如果一次写1w条，那么总IO次数确少了很多，所以这也是之前学习看各种资料的时候，都建议我们要批量写，要顺序写。\n上图也提到了，由于我们是达到指定数据大小，然后顺序写磁盘，那么当写完磁盘更新内存索引的时候就没办法做到每个ID映射1个数据的offset, 1开始我以为这样不好，没办法精确搜索具体的数据，后来想想，这其实也是好事，因为就跟之前说写1条和写1w条数据对比一样，读也是一次性多读一些数据，并不会造成太大的性能影响，相反结合热数据缓存，下次再做搜索的时候，就可以利用上缓存去提升性能。\n另外还有2个好处，在说明之前，我先说下ID和Offset索引大概是个什么样子，比如说我们有189000条数据，我们全局指定配置按5w条数据写一次磁盘，那么idOffsetMap 的数据大概会是这样\n\n\n\nKEY\nVALUE\n\n\n\n50000\n12345\n\n\n100000\n23456\n\n\n150000\n34567\n\n\n189000\n45678\n\n\n以上数据就是idOffsetMap的数据，其中Value的部分不用关心，是我随便写的，主要看Key，因为我们是5W条数据写一次磁盘，那么除了最后一个key以外，其余的部分每个Key的间隔都是5w，那么假设我实际要搜索的数据ID 为 [51234,53486,178900,180000]\n那么前2个ID会命中5w- 10w 这个区间，而后面2个ID会命中到15w-189000 这个区间，也就是说我只需要通过2次IO，就能读到我要搜索的全部数据。这是好处其中之一。\n那么如果我把上面搜索的ID数组修改一下为[51234,53486,118900,120000], 那么我们会发现，命中的2个区间是连续的，这个时候通过FileChannel的API，就可以以一次IO的方式读取，而不是多次IO。（为了减少IO次数实现顺序读，后续在查询部分会说）。\n除了idOffsetMap以外，还会有如下几个索引结构以及缓存数据的结构:\n\nhashIndexMap\nentityIndexMap\ncubeDataValueMap\nlastPageBuffer\nhashConflictDataMap\nhotPageIdList\n\n上面这几个结构里 hashIndexMap 和 entityIndexMap 在老版本的缓存里也都有，hashIndexMap里的Hash 是根据1条数据的所有维度列的值按顺序拼到一起然后通过BKDR的Hash算法算出来的，目的用来快速判断1条数据缓存是否存在。entityIndexMap 就是entity成员的值和ID列表的映射，在老版本缓存里由于不需要操作磁盘，所以那时候没有ID的概念，就是Hash映射原始数据，Entity映射Hash。\ncubeDataValueMap 这个结构是为了提高更新的速度，因为在我们的场景，update 和 delete 都是可以简单理解为对1个decimal类型的字段做更新，delete实际就是设置为NULL。那么为了避免在实际写操作的时候去写磁盘，所以需要有1个Map 去保存ID和Value 的映射关系，修改的时候只改内存，查询的时候在用内存里的value 和搜索命中的结果做合并。\nlastPageBuffer 表示的是idOffsetMap里最后一部分区间的原始数据集合，它存在的意义有2个：\n\n提高insert数据到缓存的性能，避免每次插入新数据时，都要去执行filechannel.write （并且由于之前一次写的是1批数据，是个List结构，所以追加数据前，还需要把最后一块读出来，内存里add ,在写回去。）\n提高搜索性能，如果搜索命中的区间是最后1个区间，或者说我们事实表数据量本身很小，可能就只有这1个区间，那么数据就会跟之前一样全量缓存在内存里。\n\nhashConflictDataMap 表示的是hash冲突时的原始数据和id的映射(不包含这个hash值下添加的第一条数据)，这个结构的作用是为了hash搜索时去提速用的，想象一下：我们要判断1条数据是否在缓存里存在，首先要计算这个数据所有维度列拼在一起的Hash值，然后命中到Hash索引上，如果这个索引里，这个Hash值映射了3个ID，那么我就可以优先判断后2个数据是否有我要搜索的结果（因为后2个ID的原始数据存在了hashConflictDataMap里），如果有就可以直接返回，没有的话，才需要去读取第一个ID进行搜索。由于考虑到实际hash冲突的结果不会太多，所以这部分数据也直接存在了内存里。另外在其他搜索场景里，如果ID在这个结构里作为Key存在，也会优先从这里查。\n最后1个hotPageIdList 也是为了搜索加速产生的结构，在搜索的时候，会根据搜索的条件通过内存中的索引，得到搜索的ID数组，而这些ID数组，就会计算出分别命中了哪些区间，而在读取这个区间里数据的时候，实际也是通过caffine去读取的，如果或许在查询同1个区间，就会命中缓存，而这个结构存储的是具体的区间ID（也就是pageId,代码里都是以page命名），好方便在清除缓存的时候，根据这里存的区间ID进行拼接缓存Key，把这些原始数据缓存Key也同步清理掉。还拿上面举的idOffsetMap的数据做例子，那么的值就是 50000、100000、150000、189000 这样的值。\n最后在说一个细节就是，这里的Map和List，我大量使用了eclipse-collection里的结构，底层保存的都是int 数组，避免使用传统JDK原生的集合导致的装箱，浪费内存，实测内存能节省将近10倍。\n搜索搜索的时候，首先要判断，是否能命中索引，如果命中不了，就会根据idOffsetMap 里存的offset直接循环搜索文件，然后内存过滤，就没什么好说的，我们这里主要说下命中索引的场景。\n如果能命中索引，那么最终我们就会得到1个搜索的id数组，之前也说过，会根据这个id的数组去计算搜索的区间，现在这里说下具体的细节\n\n首先判断搜索的ID，是否在hashConflictDataMap 里存在，如果存在，直接从这里查\n然后判断搜索的ID如果是在最后1个区间，那么直接内存搜索就好了。这里面还有1个很有意思的点就是，我们可以直接根据ID去算出它在lastPageBuffer的index,然后直接list.get 就好了，算法大概如下 (id % pageLoadSize) - 1 , 这里的pageLoadSize 就是之前我们积攒多少数据写一次磁盘以及idOffsetMap里区间之间的间隔，-1 是因为索引是从0开始。比如说如果我要搜索的ID是:178900 ,那么算出的index 就是 28899 。\n然后剩下的搜索ID数组，就是需要去计算区间的，计算区间的目的之前也讲过，是为了最大限度实现顺序IO，它的结构如下:\n\nIntObjectHashMap&lt;IntObjectHashMap&lt;IntArrayList&gt;&gt; \n\n以上是eclipse-collections 的声明，如果用Java内置的结构表示那么就是:\nMap&lt;Integer,Map&lt;Integer,List&lt;Integer&gt;&gt;&gt;\n\n首先最外层的Key表示的是连续读取区间的起始区间ID，里面的Map表示的是，每个区间ID和要搜索的ID列表的映射。这里用eclipse-collections去实现，也是为了避免装箱，导致过多的内存占用。\n有了这个接口，我就可以遍历上面Map的每个Key，然后用子Map里所有key 的集合去拼caffeine里热数据的缓存Key集合，然后一起传给caffeine，在caffeine的内部会帮我们判断，只有在key对应的数据在缓存里不存在的时候才会走到缓存的回调函数里，进行实际数据的IO查询。而也是由于caffeine做了这个事情，会导致1个问题，之前我们计算好连续的页，可能不连续了，比如说假设之前搜索的ID列表命中了5w-10w,10w-15w,15w-20w,20w,-25w 这几个区间，那么如果说15w-20w数据本身在缓存里存在，那么走到回调函数的Key，就只有5w-10w,10w-15w,20w,-25w 这3个区间了，也就没办法利用FileChannel的API做一次IO的读取，所以这里，我们必须要再次做一遍连续期间的计算分组，为了节省内存，这里计算的结果是1个int[][]的二位数组，外层表示分组，里面表示连续的区间ID。然后再循环这个二位数组，去进行文件的读取，以实现最大化顺序IO的效果。\n优化杂项定位搜索区间由于数据除了最后1个区间，前几个间隔都是固定的，所以可以直接用 如下公式计算到大于ID里最小ID的区间index, 然后这个区间ID和上一个区间ID组成的范围里就一定包含ID这条数据。pageLoadSize 之前说过就是idOffsetMap里的区间间隔。\nindex &#x3D; id &#x2F; pageLoadSize\n\n计算搜索区间的大小这里有2种情况\n\n如果区间id ==  pageLoadSize ，那么就是第一页，idOffsetMap.get(id) 就是要搜索区间的大小\n否则就是idIndexMap.get(id) - idIndexMap.get(id - pageLoadSize) 表示的是ID表示的当前页的offset - 上一页的offset\n\n由于最后一页的搜索是直接在lastPageBuffer里搜，所以不需要考虑最后一页的问题。\n缓存的设计在热的期间原始数据缓存里，我刻意对caffine调用了softValues 方法，以软引用的方式避免热数据不会导致JVM内存爆炸。\n在做这个设计之前，我查过相关资料，其实caffeine作者是不建议通过软引用的方式驱逐Key,除了这样目前也没有太好的办法来去做这个事情，因为caffeine只能设置最大的Key的数量，但是设置不了最大占几个G的内存，而我又不好根据Key的大小去预估最多能存多少Key。\n软引用的方式我实际有做过测试，当内存满的时候，会主动触发Key的驱逐，不会导致内存爆掉。\n锁的策略之前由于更新DB和更新缓存是独立控制的，所以无法保证数据的一致性，可能会发生更新完DB，缓存还没来得及更新的时候，另外1个线程读到了缓存的旧数据发生BUG。\n在这次的实现里通过Java8的StampedLock 实现读写锁的控制，之所以用StampedLock 是考虑它支持乐观读，性能应该会高一些。\n具体加锁的策略是，缓存内部只加读锁，写锁由外部调用控制，把更新DB和更新缓存锁在一起，这样当发生写的时候，读请求会阻塞，直到写操作完事，这样可以保证我们读到的数据是一个一致性的数据，不会发生比如说hashIndexMap是更新后的但是idOffsetMap是更新前的。\n","slug":"文件缓存的设计","date":"2022-06-05T06:55:29.000Z","categories_index":"","tags_index":"Java,工作","author_index":"李志博的博客"},{"id":"9f14b7d15ba15e4a4bdd3158410a83f4","title":"Mockito跟SpringBoot@Autowired组合使用方式","content":"有的时候，我们想绝大多数Bean在单测的时候，还是希望走正常的Bean注入,只有少部分Bean需要自己人工Mock结果，这个时候如何优雅的Mock Bean实例并自动和其他Bean实例一样进行注入呢？\n可以通过Mockito的@MockBean注解解决，代码如下\n@SpringBootTest\nclass CubeDataCacheServiceTest &#123;\n\n    @Autowired\n    private CubeDataCacheService cubeDataCacheService;\n\n    @MockBean\n    private DimensionLocalService dimensionLocalService;\n&#125;\n\n以上代码把一些不相关的代码都去掉了，接着我们就可以正常的调mockito的API，进行正常的设置mock结果就好了。\n","slug":"Mockito跟SpringBoot-Autowired组合使用方式","date":"2022-06-04T06:10:53.000Z","categories_index":"","tags_index":"mockito,SpringBoot","author_index":"李志博的博客"},{"id":"e939d2a67c854d9584266b38aea34ab3","title":"Eclipse-Collection之IntIntHashMap","content":"最近在设计1个功能，其中需要内存保存数据的全量索引，结构大概如下:\nMap&lt;Integer,Integer&gt; indexMap &#x3D; new HashMap&lt;&gt;();\n\n我跑单测观察这个Map占用内存的时候发现，110多万的数据要消耗500多MB内存，这个其实对内存的占用已经算是非常大了，那么就可以用标题说的Eclipse-Collection库的IntIntHashMap来替代之前的HashMap\n首先我们引包:\n&lt;dependency&gt;\n        &lt;groupId&gt;org.eclipse.collections&lt;&#x2F;groupId&gt;\n        &lt;artifactId&gt;eclipse-collections-api&lt;&#x2F;artifactId&gt;\n        &lt;version&gt;11.0.0&lt;&#x2F;version&gt;\n    &lt;&#x2F;dependency&gt;\n\n    &lt;dependency&gt;\n        &lt;groupId&gt;org.eclipse.collections&lt;&#x2F;groupId&gt;\n        &lt;artifactId&gt;eclipse-collections&lt;&#x2F;artifactId&gt;\n        &lt;version&gt;11.0.0&lt;&#x2F;version&gt;\n    &lt;&#x2F;dependency&gt;\n\n然后IntIntHashMap的使用和普通Map区别不大，只不过由于确定了Key和Value都是Int，所以就不需要泛型了\n@Test\nvoid printObjectSize2() throws InterruptedException &#123;\n    Thread.sleep(1000 * 60);\n    System.out.println(&quot;begin&quot;);\n    IntIntHashMap intHashMap &#x3D; new IntIntHashMap();\n    for (int i &#x3D; 0; i &lt; 1000000; i++) &#123;\n        intHashMap.put(i, i);\n    &#125;\n    System.out.println(&quot;end&quot;);\n    Thread.currentThread().join();\n&#125;\n\n执行上面的单测，begin前面的sleep是为了预留登录arthas的时间，然后通过执行memory观察当前堆内存大小。\n\n然后执行到end的时候，在执行一次memory看下堆的大小，发现这次只占用61MB。\n\n","slug":"Eclipse-Collection之IntIntHashMap","date":"2022-05-23T14:09:07.000Z","categories_index":"","tags_index":"Java,Eclipse-Collection","author_index":"李志博的博客"},{"id":"f929186e789271318e9dfdac1fb85b07","title":"native内存最佳实践","content":"堆是Java应用程序中最大的内存消耗者，但JVM将分配和使用大量本机内存。虽然第7章从编程的角度讨论了有效管理堆的方法，但堆的配置以及它如何与操作系统的本机内存交互是影响应用程序整体性能的另一个重要因素。这里存在术语冲突，因为C程序员倾向于将其本机内存的一部分称为C堆。为了与以Java为中心的世界观保持一致，我们将继续使用堆来引用Java堆，使用本机内存来引用JVM的非堆内存，包括C堆。\n本章讨论本机(或操作系统)内存的这些方面。我们首先讨论JVM的整个内存使用情况，目的是了解如何监视该使用情况以发现性能问题。然后，我们将讨论调优JVM和操作系统以优化内存使用的各种方法。\n占用空间堆(通常)占JVM使用的最大内存量，但JVM也使用内存进行其内部操作。这种非堆内存是本机内存。本机内存也可以在应用程序中分配(通过对Malloc()和类似方法的JNI调用，或者在使用新I&#x2F;O或NIO时)。JVM使用的本机内存和堆内存的总和产生了应用程序的总占用空间。\n从操作系统的角度来看，这一总占用空间是性能的关键。如果没有足够的物理内存来容纳应用程序的全部占用空间，则性能可能会开始下降。这里的关键字是可能。本机内存的一部分仅在启动期间使用(例如，与在类路径中加载JAR文件相关联的内存)，如果该内存被换出，则不一定会被注意到。一个Java进程使用的一些本机内存与系统上的其他Java进程共享，一些较小的部分与系统上的其他类型的进程共享。不过，在大多数情况下，为了获得最佳性能，您希望确保所有Java进程的总内存占用不超过机器的物理内存(另外，您还希望为其他应用程序留出一些内存)。\n测量占用空间要测量进程的总内存使用量，您需要使用特定于操作系统的工具。在基于Unix的系统中，top和ps等程序可以显示基本级别的数据；在Windows系统中，您可以使用perfmon或VMMap。无论使用哪种工具和平台，都需要查看进程的实际分配内存(而不是驻留内存)。\n分配内存和保留内存之间的区别取决于JVM(和所有程序)管理内存的方式。考虑使用参数-Xms512m  -Xmx2048m指定的堆。堆从使用512MB开始，它将根据需要调整大小，以满足应用程序的GC目标。\n这一概念是已提交(或已分配)内存和驻留内存(有时称为进程的虚拟大小)之间的本质区别。JVM必须告诉操作系统它可能需要多达2 GB的内存用于堆，以便驻留内存：操作系统承诺，当JVM增加堆的大小时尝试分配额外的内存时，该内存将是可用的。\n尽管如此，最初只分配了512MB内存，这512MB是(堆)正在使用的所有内存。该(实际分配的)内存称为提交内存。提交的内存量将随着堆大小的调整而波动；具体地说，随着堆大小的增加，提交的内存量也相应增加。\n超额预定是个问题吗？\n当我们查看性能时，只有提交的内存才是真正重要的：性能问题从来不是由于保留太多内存而导致的。\n但是，有时您希望确保JVM不会保留太多内存。对于32位JVM来说尤其如此。由于32位应用程序的最大进程大小为4 GB(或更小，具体取决于操作系统)，因此过度保留内存可能是个问题。为堆保留3.5 GB内存的JVM只剩下0.5 GB的本机内存用于堆栈、代码缓存等。如果堆扩展到只提交1 GB的内存，这并不重要：由于3.5 GB的保留，用于其他操作的内存量被限制为0.5 GB。\n64位JVM不受进程大小的限制，但受机器上的虚拟内存总量的限制。假设您有一台具有4 GB物理内存和10 GB虚拟内存的小型服务器，并启动一个最大堆大小为6 GB的JVM。这将保留6 GB的虚拟内存(外加更多的非堆内存段)。无论堆变得多大(以及提交的内存有多大)，这个JVM只能在该机器上保留不到4 GB的内存。\n在所有条件相同的情况下，增加JVM结构的大小并让JVM以最佳方式使用该内存是很方便的。但这并不总是可行的。\n这种差异几乎适用于JVM分配的所有重要内存。随着更多代码的编译，代码缓存从初始值增长到最大值。元空间与堆分开分配，并在其初始(提交)大小和最大(保留)大小之间增长。\n线程堆栈是一个例外。每次JVM创建线程时，操作系统都会分配一些本机内存来保存该线程的堆栈，从而将更多内存分配给进程(至少在线程退出之前)。不过，线程堆栈在创建时是完全分配的。\n在Unix系统中，应用程序的占用空间可以通过各种操作系统工具报告的进程的驻留集大小(RSS)来估计。该值很好地估计了一个进程正在使用的已提交内存量，尽管它在两个方面并不准确。首先，JVM和其他进程之间在操作系统级别共享的少数几个页面(即共享库的文本部分)将计入每个进程的RSS。其次，一个进程在任何时刻提交的内存都可能比它调入的内存多。不过，跟踪进程的RSS是监视总内存使用的一种很好的首选方法。在较新的Linux内核上，PSS是RSS的改进，它删除了与其他程序共享的数据。\n在Windows系统上，等同的概念称为应用程序的工作集，这是任务管理器报告的内容。\n最大限度地减少占用空间要最大限度地减少JVM使用的内存使用量，请限制以下各项使用的内存量：\n\n堆\n堆是最大的内存块，尽管令人惊讶的是它可能只占用总内存的50%到60%。使用较小的最大堆(或设置GC调优参数以使堆永远不会完全扩展)会限制程序的内存占用。\n\n\n\n线程堆栈\n线程堆栈非常大，特别是对于64位JVM而言。\n\n\n\n代码缓存\n代码缓存使用本机内存来保存编译后的代码。正如在第4章中所讨论的，这是可以调优的(尽管如果由于空间限制而无法编译所有代码，则性能将受到影响)。\n\n\n\n本机库分配\n本机库可以分配它们自己的内存，这有时可能非常重要。\n\n\n接下来的几节将讨论如何监控和减少这些区域。\n快速总结\n\nJVM的总内存占用对其性能有很大影响，特别是在机器上的物理内存受限的情况下。内存占用是性能测试的另一个应该通常监视的方面。\n\n\n本机内存跟踪JVM提供了有关它如何分配本机内存的有限可见性。重要的是要认识到，这种跟踪适用于由代码JVM本身分配的内存，但不包括由应用程序使用的本机库分配的任何内存。这既包括第三方本机库，也包括JDK本身附带的本机库(例如，libsocket.so)。\n使用选项-XX:NativeMemoryTracking=*off|summary|detail可以实现这种可见性。默认情况下，本机内存跟踪(NMT)处于关闭状态。如果开启了汇总或明细模式，您可以随时从jcmd获取本机内存信息：\n如果使用参数-XX:+PrintNMTStatistics(默认情况下为FALSE)启动JVM，则当程序退出时，JVM将输出有关分配的信息。\n以下是一个JVM的汇总输出，该JVM的初始堆大小为512 MB，最大堆大小为4 GB：\nNative Memory Tracking:\n\n Total: reserved&#x3D;5947420KB, committed&#x3D;620432KB\n\n尽管JVM总共预留了5.9 GB的内存，但它使用的内存要少得多：只有620MB。这是相当典型的(这也是不特别注意操作系统工具中显示的进程的虚拟大小的原因之一，因为这只反映了内存预留)。\n这种内存使用情况如下所示。堆本身(毫不奇怪)是驻留内存中最大的部分，为4 GB。但是堆的动态大小意味着它只增长到268 MB(在本例中，堆大小是-Xms256m-Xmx4g，因此实际的堆使用量只有少量扩展)：\n-                 Java Heap (reserved&#x3D;4194304KB, committed&#x3D;268288KB)\n                            (mmap: reserved&#x3D;4194304KB, committed&#x3D;268288KB)\n\n接下来是用于保存类元数据的本机内存。同样，请注意，JVM保留的内存比用来容纳程序中的24,316个类的内存要多。此处的提交大小将从MetaspaceSize标志的值开始，并根据需要增长，直到达到MaxMetaspaceSize标志的值：\n-                     Class (reserved&#x3D;1182305KB, committed&#x3D;150497KB)\n                            (classes #24316)\n                            (malloc&#x3D;2657KB #35368)\n                            (mmap: reserved&#x3D;1179648KB, committed&#x3D;147840KB)\n\n分配了77个线程堆栈，每个线程堆栈大约1 MB：\n-                    Thread (reserved&#x3D;84455KB, committed&#x3D;84455KB)\n                            (thread #77)\n                            (stack: reserved&#x3D;79156KB, committed&#x3D;79156KB)\n                            (malloc&#x3D;243KB, #314)\n                            (arena&#x3D;5056KB, #154)\n\n然后是JIT代码缓存：24,316个类不是很多，所以只提交了一小部分代码缓存：\n-                      Code (reserved&#x3D;102581KB, committed&#x3D;15221KB)\n                            (malloc&#x3D;2741KB, #4520)\n                            (mmap: reserved&#x3D;99840KB, committed&#x3D;12480KB)\n\n接下来是GC算法用于处理的堆之外的区域。此区域的大小取决于正在使用的GC算法：(简单的)串行收集器保留的空间将远远少于更复杂的G1GC算法(尽管一般来说，这里的保留量永远不会很大)：\n-                        GC (reserved&#x3D;199509KB, committed&#x3D;53817KB)\n                            (malloc&#x3D;11093KB #18170)\n                            (mmap: reserved&#x3D;188416KB, committed&#x3D;42724KB)\n\n类似地，除了放置在代码缓存中的结果代码外，编译器还使用该区域进行其操作：\n-                  Compiler (reserved&#x3D;162KB, committed&#x3D;162KB)\n                            (malloc&#x3D;63KB, #229)\n                            (arena&#x3D;99KB, #3)\n\nJVM的内部操作在这里表示。它们中的大多数往往都很小，但有一个重要的例外是direct byte buffers，其分配如下：\n-                  Internal (reserved&#x3D;10584KB, committed&#x3D;10584KB)\n                            (malloc&#x3D;10552KB #32851)\n                            (mmap: reserved&#x3D;32KB, committed&#x3D;32KB)\n\n符号表引用(来自类文件的常量)保存在这里：\n-                    Symbol (reserved&#x3D;12093KB, committed&#x3D;12093KB)\n                            (malloc&#x3D;10039KB, #110773)\n                            (arena&#x3D;2054KB, #1)\n\nNMT本身需要一些空间来运行(这是默认情况下不启用它的原因之一)：\n-    Native Memory Tracking (reserved&#x3D;7195KB, committed&#x3D;7195KB)\n                            (malloc&#x3D;16KB #199)\n                            (tracking overhead&#x3D;7179KB)\n\n最后，以下是JVM的一些次要记账部分：\n-               Arena Chunk (reserved&#x3D;188KB, committed&#x3D;188KB)\n                            (malloc&#x3D;188KB)\n-                   Unknown (reserved&#x3D;8192KB, committed&#x3D;0KB)\n                            (mmap: reserved&#x3D;8192KB, committed&#x3D;0KB)\n\n\n详细的内存跟踪信息\n如果JVM以-XX:NativeMemoryTracking=detail启动，那么jcmd(带有最后一个detail参数)将提供有关本机内存分配的详细信息。这包括整个内存空间的映射，其中包括如下行：\n0x00000006c0000000 - 0x00000007c0000000] reserved 4194304KB for Java Heap\n        from [ReservedSpace::initialize(unsigned long, unsigned long,\n                            bool, char*, unsigned long, bool)+0xc2]\n        [0x00000006c0000000 - 0x00000006fb100000] committed 967680KB\n            from [PSVirtualSpace::expand_by(unsigned long)+0x53]\n        [0x000000076ab00000 - 0x00000007c0000000] committed 1397760KB\n            from [PSVirtualSpace::expand_by(unsigned long)+0x53]\n\n这4 GB的堆空间是在初始化()函数中保留的，其中两次分配是在Expand_by()函数中进行的。\n这种信息会在整个进程空间中重复出现。如果您是JVM工程师，它提供了一些有趣的线索，但对于我们其他人来说，摘要信息足够有用了。\n总体而言，NMT提供了两条关键信息：\n\n提交的总大小\nJVM的总提交大小(理想情况下)接近进程将消耗的物理内存量。反过来，这应该接近应用程序的RSS(或工作集)，但是那些由操作系统提供的度量不包括任何已提交但被调出进程的内存。事实上，如果进程的RSS小于提交的内存，这通常表示操作系统难以在物理内存中容纳所有JVM。\n\n\n\n个人提交大小\n当需要调优堆、代码缓存和元空间的最大值时，了解JVM使用了多少内存是很有帮助的。过度分配这些区域通常只会导致无害的内存保留，尽管当保留的内存很重要时，NMT可以帮助跟踪哪些地方可以削减这些最大大小。\n\n\n另一方面，正如我在本节开头所指出的，NMT不提供对共享库的本机内存使用的可见性，因此在某些情况下，总进程大小将大于JVM数据结构的提交大小。\nNMT随时间推移\nNMT还允许您跟踪一段时间内内存分配是如何发生的。在启用NMT的情况下启动JVM后，您可以使用以下命令建立内存使用基准：\njcmd process_id VM.native_memory baseline\n\n这会导致JVM标记其当前的内存分配。稍后，您可以将当前内存使用情况与该标记进行比较：\n% jcmd process_id VM.native_memory summary.diff\nNative Memory Tracking:\n\nTotal:  reserved&#x3D;5896078KB  -3655KB, committed&#x3D;2358357KB -448047KB\n\n-             Java Heap (reserved&#x3D;4194304KB, committed&#x3D;1920512KB -444927KB)\n                        (mmap: reserved&#x3D;4194304KB, committed&#x3D;1920512KB -444927KB)\n\n在本例中，JVM保留了5.8 GB的内存，目前正在使用2.3 GB。承诺的大小比建立基线时减少了448 MB。类似地，堆使用的提交内存减少了444MB(可以检查输出的其余部分，以查看内存使用量减少的其他地方占剩余的4MB)。\n这是一种检查JVM随时间变化的内存占用情况的有用技术。\n\nNMT自动禁用\n在NMT输出中，我们看到NMT本身需要本机内存。此外，启用NMT将创建帮助进行内存跟踪的后台线程。\n如果JVM的内存或CPU资源变得非常紧张，NMT将自动关闭以节省资源。这通常是一件好事–除非你需要诊断的是紧张的情况。在这种情况下，您可以通过禁用   -XX:-AutoShutdownNMT NMT标志(默认情况下为true)来确保NMT继续运行。\n\n快速总结\n\n本机内存跟踪(NMT)提供有关JVM本机内存使用的详细信息。从操作系统的角度来看，这包括JVM堆(对于操作系统来说，它只是本机内存的一部分)。\nNMT的摘要模式足以进行大多数分析，并允许您确定JVM已经提交了多少内存(以及该内存用于什么)。\n\n\n共享库本机内存从体系结构的角度来看，NMT是HotSpot的一部分：运行应用程序的Java字节码的C++引擎。这是在JDK本身之下的，因此它不在JDK级别跟踪任何东西的分配。这些分配来自共享库(由System.loadLibrary()调用加载的库)。\n共享库通常被认为是Java的第三方扩展：例如，Oracle WebLogic Server有几个本机库，它使用它们比JDK更有效地处理I&#x2F;O。但JDK本身有几个本机库，与所有共享库一样，这些库都不在NMT的视野中。\n因此，NMT通常不会检测到本机内存泄漏–即应用程序的RSS或工作集不断超时增长。NMT监控的内存池通常都有一个上限(例如，最大堆大小)。NMT在告诉我们哪些池正在使用大量内存(因此哪些需要调优以使用更少的内存)方面很有用，但是不加绑定地泄漏本机内存的应用程序通常是因为本机库中的问题而这样做的。\n没有Java级别的工具可以真正帮助我们检测应用程序正在使用共享库中的本机内存的位置。操作系统级别的工具可以告诉我们，进程的工作集正在不断增长，如果进程增长到有10 GB的工作集，而NMT告诉我们JVM只分配了6 GB的内存，我们就知道其他4 GB的内存必须来自本地库分配。\n找出哪个本地库负责需要操作系统级别的工具，而不是来自JDK的工具。各种调试版本的malloc可用于此目的。这些在一定程度上是有用的，尽管本机内存通常是通过mmap调用来分配的，而大多数用于跟踪malloc调用的库都会错过这些。\n一个很好的替代方案是一个分析器，它可以分析本机代码和Java代码。例如，在第3章中，我们讨论了Oracle Studio Profiler，这是一个混合语言的Profiler。该分析器还具有跟踪内存分配的选项–需要注意的是，它只能跟踪本机代码的内存分配，而不能跟踪Java代码的内存分配，但这正是我们在本例中要考虑的问题。\n图8-1显示了Studio Profiler中的本机分配视图。\n\n这个调用图向我们展示了WebLogic本机函数mapFile已经使用mmap为我们的进程分配了大约150 GB的本机内存。这有点误导：存在到该文件的多个映射，而分析器不够智能，无法意识到它们共享的是实际内存：例如，如果15 GB文件有100个映射，则内存使用量仅增加15 GB。(坦率地说，我故意破坏了该文件，使其变得如此大；这根本不能反映实际使用情况。)尽管如此，本地分析器已经指出了问题的位置。\n在JDK本身内，有两个常见的操作可能会导致大量本机内存使用：使用Inflater和Deflater对象，以及使用NIO缓冲区。即使不进行性能分析，也有方法可以检测这些操作是否导致本机内存增长。\n本地内存和inflaters&#x2F;deflatersinflaters和deflaters执行各种压缩：ZIP、GZIP等。它们可以直接使用，也可以通过各种输入流隐式使用。这些不同的算法使用特定于平台的本地库来执行它们的操作。这些库可以分配大量的本机内存。\n当您使用这些类中的一个时，正如文档所述，您应该在操作完成时调用end()方法。除其他事项外，这还释放了对象使用的本机内存。如果您正在使用流，则应该关闭流(并且流类将在其内部对象上调用end()方法)。\n如果您忘记调用end()方法，也不会失去一切。回想一下第7章，所有对象都有一个完全针对这种情况的清理机制：在收集Inflater时，与对象相关联的finalize()方法(在JDK 8中)或Cleaner(在JDK 11中)可以调用end()方法。因此，您不会在这里泄漏本机内存；最终，对象将被收集并最终确定，本机内存将被释放。\n不过，这可能需要很长时间。Inflater的大小相对较小，并且在具有很少执行完整GC的大堆的应用程序中，这些对象很容易升级到老一代并停留几个小时。因此，即使在技术上没有泄漏-当应用程序执行完整的GC时，本机内存最终将被释放-在这里调用end()操作的失败可能具有所有本机内存泄漏的表现。\n在这个问题上，如果Inflater本身在Java代码中泄漏，那么本机内存实际上也会泄漏。\n因此，当大量本机内存泄漏时，获取应用程序的堆转储并查找这些Inflater和deflaters会很有帮助。这些对象很可能不会在堆本身中引起问题(它们太小了，不适合堆)，但它们中的大量对象将表明存在大量的本机内存使用。\n本地NIO缓冲如果NIO字节缓冲区是通过ByteBuffer类的allocateDirect()方法或FileChannel类的map()方法创建的，则它们将分配本机(堆外)内存。\n从性能角度来看，Native ByteBuffer很重要，因为它们允许本机代码和Java代码共享数据，而无需复制数据。用于文件系统和套接字操作的缓冲区是最常见的示例。将数据写入本机NIO缓冲区，然后将该数据发送到通道(例如，文件或套接字)，不需要在JVM和用于传输数据的C库之间复制数据。如果改为使用堆字节缓冲区，则必须由JVM复制缓冲区的内容。\nallocateDirect()方法调用的开销很大；应该尽可能多地重用直接字节缓冲区。理想的情况是，当线程是独立的，并且每个线程都可以保留一个直接的字节缓冲区作为线程局部变量时。如果许多线程需要不同大小的缓冲区，这有时会使用过多的本机内存，因为最终每个线程都会得到一个最大可能大小的缓冲区。对于这种情况–或者当线程本地缓冲区不适合应用程序设计时–直接字节缓冲区的对象池可能更有用。\n也可以通过对字节缓冲区进行切片(Slice)来管理它们。应用程序可以分配一个非常大的直接字节缓冲区，并且各个请求可以使用ByteBuffer类的Slice()方法从该缓冲区中分配一部分。当片的大小不总是相同时，此解决方案可能会变得笨拙：原始字节缓冲区可能会变成碎片，就像分配和释放不同大小的对象时堆会变成碎片一样。然而，与堆不同的是，字节缓冲区的各个片不能被压缩，因此只有当所有片的大小都一致时，该解决方案才能很好地工作。\n从调优的角度来看，使用这些编程模型要实现的一件事是，应用程序可以分配的直接字节缓冲区空间量可以受到JVM的限制。可分配给直接字节缓冲区的内存总量通过设置-XX:MaxDirectMemorySize=*N*标志来指定。在当前JVM中，此标志的缺省值为0。该限制的含义一直是频繁更改的主题，但在Java 8的更高版本(以及Java 11的所有版本)中，最大限制等于最大堆大小：如果最大堆大小为4 GB，您还可以在直接和&#x2F;或映射字节缓冲区中创建4 GB的堆外内存。如果需要，您可以将该值增加到超过最大堆值。\n分配给直接字节缓冲区的内存包括在NMT报告的内部部分；如果该数字很大，则几乎总是因为这些缓冲区。如果您想确切地知道缓冲区本身消耗了多少，mBeans会对此进行跟踪。检查mbean java.nio.BufferPool.direct.Attributes或java.nio.BufferPool.mappd.Attributes将显示每种类型已分配的内存量。图8-2显示了一个例子，其中我们映射了10个缓冲区，总计10MB的空间。\n\n由于内存分配库的设计，大型Linux系统有时会出现本机内存泄漏。这些库将本机内存划分为分配段，这有利于由多个线程进行分配(因为它限制了锁争用)。\n但是，本机内存的管理方式与Java堆不同：尤其是，本机内存永远不会被压缩。因此，本机内存中的分配模式可能会导致与第5章中描述的相同的碎片。\nJava中的本机内存可能会因为本机内存碎片而耗尽；这种情况最常发生在较大的系统上(例如，具有八个以上内核的系统)，因为Linux中的内存分区数是系统内核数的函数。\n有两件事可以帮助诊断这个问题：首先，应用程序将抛出一个OutOfMemoyError，说明它耗尽了本机内存。其次，如果您查看该进程的smaps文件，它将显示许多小的(通常为64 KB)分配。在这种情况下，补救方法是将环境变量MALLOC_ARENA_MAX设置为一个小数字，如2或4。该变量的默认值是系统上的内核数乘以8(这就是问题在大型系统上更常见的原因)。在这种情况下，本机内存仍将是碎片，但碎片应该不会那么严重。\n快速总结\n\n如果一个应用程序似乎使用了太多的本机内存，那么它很可能来自本机库，而不是JVM本身。\n本地配置文件可以有效地确定这些分配的来源。\n几个常见的JDK类通常会导致本机内存使用；请确保正确使用这些类。\n\n\n针对操作系统的JVM调优JVM可以使用多个调优来改进其使用操作系统内存的方式。\n大页关于内存分配和交换的讨论以页为单位进行。页面是操作系统用来管理物理内存的内存单位。它是操作系统的最小分配单位：当分配1个字节时，操作系统必须分配整个页面。对该程序的进一步分配来自同一页，直到它被填满，此时将分配新的页。\n操作系统分配的页面比物理内存所能容纳的页面多得多，这就是为什么有分页：地址空间的页面被移入和移出交换空间(或其他存储，取决于页面包含的内容)。这意味着这些页面和它们当前存储在计算机RAM中的位置之间必须有某种映射。这些映射有两种处理方式。所有页面映射都保存在全局页表中(操作系统可以扫描该表以找到特定的映射)，而最常用的映射保存在转换后备缓冲区(TLB)中。TLB保存在快速缓存中，因此通过TLB条目访问页面比通过页表访问要快得多。\n机器的TLB条目数量有限，因此最大限度地提高TLB条目的命中率变得很重要(它充当最近最少使用的缓存)。由于每个条目代表一页内存，因此增加应用程序使用的页大小通常是有利的。如果每个页面代表更多的内存，则需要更少的TLB条目来包含整个程序，并且更有可能在需要时在TLB中找到页面。一般来说，这对任何程序都是正确的，对于Java应用程序服务器或其他具有中等大小堆的Java程序也是如此。\n必须同时在Java和OS级别启用大页面。在Java级别，-XX:+UseLargePages标志启用大页面使用；默认情况下，该标志为FALSE。并不是所有的操作系统都支持大页面，启用它们的方式显然各不相同。并不是所有的操作系统都支持大页面，启用它们的方式显然各不相同。\n如果在不支持大页面的系统上启用了UseLargePages标志，则不会给出警告，并且JVM使用常规页面。如果在确实支持大页面的系统上启用了UseLargePages标志，但没有大页面可用(因为它们已全部在使用中或操作系统配置错误)，则JVM将打印一条警告。\nLinux巨型(大)页面Linux将大页面称为巨型页面。Linux上巨大页面的配置因版本而异；有关最准确的说明，请参考您的发行版的文档。但一般程序如下：\n\n确定内核支持哪些巨大的页面大小。该大小基于计算机处理器和内核启动时给定的引导参数，但最常见的值为2 MB：\n\n# grep Hugepagesize &#x2F;proc&#x2F;meminfo\nHugepagesize:       2048 kB\n\n\n计算出需要多少大页面。如果JVM将分配一个4 GB的堆，并且系统有2 MB的巨大页面，那么该堆将需要2,048个巨大的页面。可以使用的巨大页面的数量是在Linux内核中全局定义的，因此对将运行的所有JVM(以及将使用巨大页面的任何其他程序)重复此过程。您应该将这个值高估10%，以说明大型页面的其他非堆使用(因此，这里的示例使用2,200个大型页面)。\n\n将该值写入操作系统(以便立即生效)：\n\n\n# echo 2200 &gt; &#x2F;proc&#x2F;sys&#x2F;vm&#x2F;nr_hugepages\n\n\n将该值保存在/etc/sysctl.conf中，以便在重新启动后保留该值：\n\nsys.nr_hugepages&#x3D;2200\n\n\n在许多版本的Linux上，用户可以分配的巨大页面内存量是有限的。编辑&#x2F;etc&#x2F;security&#x2F;limits.conf文件，并为运行您的JVM的用户(例如，在本例中为用户appuser)添加成员锁条目：\n\nappuser soft    memlock        4613734400\nappuser hard    memlock        4613734400\n\n如果修改了limits.conf文件，则用户必须重新登录才能使值生效。此时，JVM应该能够分配必要的大页面。要验证它是否正常工作，请运行以下命令：\n# java -Xms4G -Xmx4G -XX:+UseLargePages -version\njava version &quot;1.8.0_201&quot;\nJava(TM) SE Runtime Environment (build 1.8.0_201-b09)\nJava HotSpot(TM) 64-Bit Server VM (build 25.201-b09, mixed mode)\n\n该命令的成功完成表明巨大的页面配置正确。如果超大页面内存配置不正确，则会给出警告：\nJava HotSpot(TM) 64-Bit Server VM warning:\nFailed to reserve shared memory (errno &#x3D; 22).\n\n程序在这种情况下运行；它只是使用常规页面，而不是大页面。\nLinux透明巨型页面从2.6.32版开始的Linux内核支持透明的巨大页面。它们(理论上)提供了与传统巨型页面相同的性能优势，但它们与传统巨型页面有一些不同。\n首先，传统的大页面被锁定在内存中；它们永远不能交换。对于Java来说，这是一个优势，因为正如我们已经讨论过的，交换堆的一部分不利于GC性能。透明的巨大页面可能会被交换到磁盘，这不利于性能。\n其次，透明巨型页面的分配也与传统的巨型页面有很大不同。传统的巨大页面在内核引导时被搁置；它们始终可用。透明的巨大页面是按需分配的：当应用程序请求2 MB的页面时，内核将尝试在物理内存中为该页面找到2 MB的连续空间。如果物理内存是碎片化的，内核可能会决定花时间在一个类似于Java堆中的内存压缩的过程中重新安排页面。这意味着分配页面的时间可能要长得多，因为它需要等待内核完成为内存腾出空间。\n这会影响所有程序，但对于Java，它可能会导致非常长的GC暂停。在GC期间，JVM可能决定扩展堆并请求新页面。如果页面分配花费几百毫秒甚至一秒，GC时间就会受到很大影响。\n第三，透明的巨型页面在操作系统和Java级别上的配置都不同。具体内容如下。\n在操作系统级别，通过更改&#x2F;sys&#x2F;core&#x2F;mm&#x2F;transparent_hugepage&#x2F;enabled的内容来配置透明巨型页面：\n# cat &#x2F;sys&#x2F;kernel&#x2F;mm&#x2F;transparent_hugepage&#x2F;enabled\nalways [madvise] never\n# echo always &gt; &#x2F;sys&#x2F;kernel&#x2F;mm&#x2F;transparent_hugepage&#x2F;enabled\n# cat &#x2F;sys&#x2F;kernel&#x2F;mm&#x2F;transparent_hugepage&#x2F;enabled\n[always] madvise never\n\n这里的三个选项如下：\n\nalways\n在可能的情况下，所有程序都会有很大的页面。\n\n\n\nmadvise\n请求巨大页面的程序会被提供给它们；其他程序则会获得常规的(4KB)页面。\n\n\n\nnever\n没有程序会得到很大的页面，即使当他们请求它们时也是如此。\n\n\n不同版本的Linux对该设置有不同的缺省值(它可能会在未来的版本中更改)。例如，Ubuntu 18.04 LTS将缺省值设置为madvise，但CentOS 7(以及基于此的Red Hat和Oracle Enterprise Linux等供应商版本)将其设置为Always。还要注意，在云计算机上，OS镜像的供应商可能已经更改了该值；我看到Ubuntu镜像也将该值设置为Always。\n如果将该值设置为Always，则不需要在Java级别进行配置：将为JVM提供巨大的页面。事实上，在该系统上运行的所有程序都将在巨大的页面中运行。\n如果将该值设置为madvise，并且您希望JVM使用大型页面，请指定UseTransparentHugePages标志(默认情况下为FALSE)。然后，当JVM分配页面并获得巨大的页面时，它将发出适当的请求。\n可以预见的是，如果将该值设置为Never，则任何Java级参数都不会允许JVM获得巨大的页面。然而，与传统的大页面不同，如果您指定了UseTransparentHugePages标志，并且系统无法提供它们，则不会给出警告。\n由于透明巨型页面在交换和分配方面的不同，通常不建议在Java中使用它们；当然，使用它们可能会导致不可预测的暂停时间峰值。另一方面，特别是在默认情况下启用它们的系统上，您将在使用它们的大部分时间内透明地看到性能优势。但是，如果您想确保在大型页面中获得最流畅的性能，最好将系统设置为仅在请求时使用透明的大型页面，并配置传统的大型页面以供JVM使用。\n总结尽管Java堆是最受关注的内存区域，但JVM的整个占用空间对其性能至关重要，尤其是与操作系统相关的性能。本章中讨论的工具允许您跟踪随时间变化的内存占用情况(最重要的是，将重点放在JVM的已提交内存而不是驻留内存上)。\nJVM使用OS内存的某些方式–尤其是大页面–也可以调优以提高性能。长时间运行的JVM几乎总是能从使用大页面中受益，尤其是在它们拥有大堆的情况下。\n","slug":"native内存最佳实践","date":"2022-05-21T01:43:03.000Z","categories_index":"","tags_index":"Java,JVM","author_index":"李志博的博客"},{"id":"56a9693faf1b310ae2faaa7452bad4b8","title":"高级文件IO","content":"通常，人们会对CPU及其性能感到紧张。虽然很重要，但在许多实际应用程序工作负载中，拖累性能的不是CPU，而是I&#x2F;O代码路径。这是完全可以理解的；回想一下，在第2章-虚拟内存中，我们展示了与RAM相比，磁盘速度慢了几个数量级。这种情况与网络I&#x2F;O类似；因此，由于大量持续的磁盘和网络I&#x2F;O，自然会出现真正的性能瓶颈。\n在本章中，读者将学习几种提高I&#x2F;O性能的方法；一般而言，这些方法包括：\n\n充分利用内核PageCache\n向内核提供有关文件使用模式的提示和建议\n使用scatter-gather(向量化)I&#x2F;O\n利用内存映射进行文件I&#x2F;O\n了解和使用复杂的DIO和AIO技术。\n了解I&#x2F;O调度程序\n用于监视、分析和控制I&#x2F;O带宽的工具&#x2F;API&#x2F;CGroup。\n\nI&#x2F;O性能建议执行I&#x2F;O时的关键点是意识到底层存储(磁盘)硬件比RAM慢得多。因此，设计策略以最大限度地减少对磁盘的访问，并从内存中进行更多工作将总是有帮助的。事实上，在库的层面上(我们已经详细讨论过缓冲)和操作系统(通过PageCache和块I&#x2F;O层中的其他功能，事实上，甚至在现代硬件中)都将执行大量工作来确保这一点。对于(系统)应用程序开发人员，接下来将提出一些需要考虑的建议。\n如果可行，在对文件执行I&#x2F;O操作时使用较大的缓冲区(用于保存读取或写入的数据)，但有多大？一个很好的经验法则是对本地缓冲区使用与文件所在文件系统的I&#x2F;O块大小相同的大小(事实上，此字段在内部记录为文件系统I&#x2F;O的块大小)。查询它很简单：对要在其中执行I&#x2F;O的文件发出stat(1)命令。例如，假设在Ubuntu 18.04系统上，我们希望读入当前运行的内核的配置文件的内容：\n$ uname -r\n4.15.0-23-generic\n$ ls -l &#x2F;boot&#x2F;config-4.15.0-23-generic \n-rw-r--r-- 1 root root 216807 May 23 22:24 &#x2F;boot&#x2F;config-4.15.0-23-generic\n$ stat &#x2F;boot&#x2F;config-4.15.0-23-generic \n File: &#x2F;boot&#x2F;config-4.15.0-23-generic\n Size: 216807 Blocks: 424 IO Block: 4096 regular file\nDevice: 801h&#x2F;2049d Inode: 398628 Links: 1\nAccess: (0644&#x2F;-rw-r--r--) Uid: ( 0&#x2F; root) Gid: ( 0&#x2F; root)\nAccess: 2018-07-30 12:42:09.789005000 +0530\nModify: 2018-05-23 22:24:55.000000000 +0530\nChange: 2018-06-17 12:36:34.259614987 +0530\n Birth: -\n\n从代码中可以看出，stat(1)揭示了内核中文件的inode数据结构的几个文件特征(或属性)，其中包括I&#x2F;O块大小。\n在内部，stat(1)实用程序发出stat(2)系统调用，该调用解析底层文件的inode并将所有详细信息提供给用户空间。因此，当以编程方式需要时，使用[f]stat(2)API。\n此外，如果内存不是一个限制，为什么不分配一个中等到非常大的缓冲区并通过它执行I&#x2F;O；这会有帮助的。确定有多大需要在目标平台上进行一些调查；让您了解一下，在早期，管道I&#x2F;O使用一页大小的内核缓冲区；在现代的Linux内核上，管道I&#x2F;O缓冲区大小默认增加到1 MB。\nPage Cache当进程(或线程)通过使用Fread(3)或fWRITE(3)库层API执行文件I&#x2F;O时，它们最终通过Read(2)和WRITE(2)系统调用发布给底层操作系统。这些系统调用让内核执行I&#x2F;O；尽管这看起来很直观，但实际上读写系统调用并不同步；也就是说，它们可能会在实际I&#x2F;O完成之前返回。(显然，对文件的写入将是这种情况；同步读取必须将读取的数据返回到用户空间内存缓冲区；在此之前，读取是阻塞的。然而，使用异步I&#x2F;O(AIO)，甚至可以进行异步读取。)\n事实上，在内核中，每个单文件I&#x2F;O操作都缓存在称为PageCache的全局内核缓存中。因此，当进程将数据写入文件时，数据缓冲区不会立即刷新到底层块设备(磁盘或闪存)，而是缓存在PageCache中。类似地，当进程从底层块设备读取数据时，数据缓冲区不会立即复制到用户空间进程内存缓冲区；不，您猜对了，它首先存储在PageCache中(进程实际上将从那里接收数据)。\n\n\n为什么内核的PageCache中的这种缓存是有帮助的？很简单：通过利用缓存的关键属性，即缓存内存区域(RAM)和缓存区域(块设备)之间的速度差异，我们可以获得极高的性能。PageCache位于RAM中，因此，当应用程序对文件数据执行读取时，(尽可能)保持所有文件I&#x2F;O的内容被缓存几乎可以保证对缓存的命中；从RAM读取要比从存储设备读取快得多。类似地，内核不是缓慢而同步地将应用程序数据缓冲区直接写入块设备，而是将写数据缓冲区缓存在PageCache中。显然，将写入的数据刷新到底层块设备以及管理PageCache内存本身的工作完全在Linux内核的工作范围内(我们在这里不讨论这些内部细节)。\n向内核提供有关文件I&#x2F;O模式的提示我们现在了解到，内核继续将所有文件I&#x2F;O缓存在其页面缓存中；这对性能有好处。思考一个例子是很有用的：应用程序设置并对非常大的视频文件执行流读取(以在某个应用程序窗口中向用户显示它；我们将假设该特定视频文件是第一次被访问)。很容易理解，一般来说，在从磁盘读取文件时缓存文件会有所帮助，但在这里，在这种特殊情况下，它不会有太大帮助，因为第一次，我们仍然必须首先到磁盘上读取它。因此，我们耸耸肩，继续以通常的方式对其进行编码，顺序读取视频数据块(通过其底层编解码器)，并将其传递给呈现代码。\n通过POSIX_fise(2)API我们能做得更好吗？的确如此：Linux提供了posix_fadvise系统调用，允许应用程序进程通过一个名为advise的参数向内核提供有关其文件数据访问模式的提示。与我们的示例相关的是，我们可以将通知作为值POSIX_FADV_SEQUENCE、POSIX_FADV_WILLNEED传递，以通知内核我们希望按顺序读取文件数据，并且我们希望在不久的将来需要访问文件数据。该建议会导致内核按顺序(从低到高的文件偏移量)对内核页面缓存启动积极的文件数据预读。这将极大地帮助提高性能。\nposix_fadvise系统调用的签名如下：\n#include &lt;fcntl.h&gt;\nint posix_fadvise(int fd, off_t offset, off_t len, int advice);\n\n显然，第一个参数fd表示文件描述符，第二个和第三个参数offset和len指定文件的一个区域，我们通过第四个参数advice在该区域上传递提示或建议。(长度实际上向上舍入为页面粒度。)\n不仅如此，在完成对视频数据块的处理后，应用程序甚至可以通过调用POSIX_FADVDONTNEED值设置为POSIX_FADV_DONTNEED值的POSIX_FADVE来向操作系统指定它将不再需要该特定的内存块；这将是对内核的一个提示，它可以释放保存该数据的页面缓存的页面，从而为传入的重要数据(以及可能仍然有用的已经缓存的数据)创造空间。\n有一些需要注意的事项。首先，对于开发人员来说，重要的是要认识到这个建议实际上只是对操作系统的一个提示和建议；它可能会得到尊重，也可能不会得到尊重。接下来，同样，即使目标文件的页面被读取到页面缓存中，它们也可能因为各种原因而被逐出，内存压力是典型的原因。不过，尝试一下也没什么坏处；内核通常会考虑这些建议，而且它确实可以提高性能。(可以像往常一样在与此API相关的手册页中查找更多建议值。)\n通过readahead API在执行积极的文件预读方面，特定于Linux(GNU)的readahead系统调用实现了与我们刚才看到的posix_fadvise类似的结果。其签名如下：\ninclude &lt;fcntl.h&gt;\nssize_t readahead(int fd, off64_t offset, size_t count);\n\n在fd指定的目标文件上执行预读，从文件偏移量开始，最大计数字节数(四舍五入到页面粒度)。\n使用pread 、pwrite API的MT应用程序文件I&#x2F;O回想一下read和write系统调用；它们构成了对文件执行I&#x2F;O的基础。您还会记得，在使用这些API时，操作系统将隐式更新底层文件偏移量。例如，如果进程打开一个文件(通过open)，然后执行512字节的read，则文件的偏移量(或所谓的查找位置)现在将是512。如果它现在写入比方说200字节，则写入将从位置512发生到位置712，从而将新的寻道位置或偏移量设置为该数字。\n好吧，那又怎样？我们的观点很简单，当多线程应用程序有多个线程同时在同一底层文件上执行I&#x2F;O时，隐式设置文件的偏移量会导致问题。但请稍等，我们之前已经提到过这一点：需要锁定文件，然后再对其进行操作。但是，锁定会造成主要的性能瓶颈。如果你设计了一个MT应用程序，它的线程并行地处理同一文件的不同部分，会怎么样？这听起来很棒，只是文件的偏移量会不断变化，从而破坏我们的并行性，从而破坏性能。\n那么，你是做什么的？Linux为此提供了pread和pwrite系统调用(p表示定位的I&#x2F;O)；使用这些API，可以指定(或定位)执行I&#x2F;O的文件偏移量，并且操作系统不会更改实际的底层文件偏移量。他们的签名如下：\n#include &lt;unistd.h&gt;\nssize_t pread(int fd, void *buf, size_t count, off_t offset);\nssize_t pwrite(int fd, const void *buf, size_t count, off_t offset);\n\npread&#x2F;pwrite和通常的read&#x2F;write系统调用之间的区别在于，前面的API采用额外的第四个参数-执行读或写I&#x2F;O操作的文件偏移量，而不修改它。\n这使我们能够实现我们想要的：通过让多个线程同时并行读写文件的不同部分，让MT应用程序执行高性能I&#x2F;O。\n需要注意的几点：首先，与read和write一样，pread和pwrite也可以在没有传输所有请求的字节的情况下返回；程序员有责任在循环中检查和调用API，直到没有剩余的字节要传输。正确使用读&#x2F;写API，其中解决了此类问题。其次，当使用指定的O_APPEND标志打开文件时，Linux的pwrite系统调用总是将数据附加到EOF，而不考虑当前的偏移量；这违反了POSIX标准，该标准规定O_APPEND标志不应对发生写入的起始位置产生影响。第三，非常明显(但我们必须声明)，被操作的文件必须能够被搜索(即，支持fseek或lseekAPI)。常规文件始终支持查找操作，但管道和某些类型的设备不支持。\nScatter （分散）– gather（聚集） I&#x2F;O为了帮助解释这个主题，假设我们被委托将数据写入文件，从而写入三个不连续的数据区域A、B和C(分别用AS、BS和Cs填充)；下图显示了这一点：\n\n\n注意文件是如何有洞的–不包含任何数据内容的区域；这可以使用常规文件(主要是空洞的文件称为稀疏文件)来实现。你是如何创建这个洞的？很简单：只需执行lseek，然后write数据；向前搜索的长度决定了文件中漏洞的大小。那么，我们如何实现所示的数据文件布局呢？我们将展示两种方法–一种是传统方式，另一种是更优化的性能方法。让我们从传统的方法开始。\n非连续数据文件-传统方法这似乎很简单：首先查找到所需的起始偏移量，然后写入所需长度的数据内容；这可以通过一对lseek和write系统调用完成。当然，我们必须调用这对系统调用三次。因此，我们编写了一些代码来实际执行此任务；请参见此处的代码(相关片段)(ch18&#x2F;sgio_imple.c)：\n#define A_HOLE_LEN  10\n#define A_START_OFF A_HOLE_LEN\n#define A_LEN       20\n\n#define B_HOLE_LEN  100\n#define B_START_OFF (A_HOLE_LEN+A_LEN+B_HOLE_LEN)\n#define B_LEN        30\n\n#define C_HOLE_LEN  20\n#define C_START_OFF (A_HOLE_LEN+A_LEN+B_HOLE_LEN+B_LEN+C_HOLE_LEN)\n#define C_LEN       42\n...\nstatic int wr_discontig_the_normal_way(int fd)\n&#123; ...\n    &#x2F;* A: &#123;seek_to A_START_OFF, write gbufA for A_LEN bytes&#125; *&#x2F;\n    if (lseek(fd, A_START_OFF, SEEK_SET) &lt; 0)\n        FATAL(&quot;lseek A failed\\n&quot;);\n    if (write(fd, gbufA, A_LEN) &lt; 0)\n        FATAL(&quot;write A failed\\n&quot;);\n\n    &#x2F;* B: &#123;seek_to B_START_OFF, write gbufB for B_LEN bytes&#125; *&#x2F;\n    if (lseek(fd, B_START_OFF, SEEK_SET) &lt; 0)\n        FATAL(&quot;lseek B failed\\n&quot;);\n    if (write(fd, gbufB, B_LEN) &lt; 0)\n        FATAL(&quot;write B failed\\n&quot;);\n\n    &#x2F;* C: &#123;seek_to C_START_OFF, write gbufC for C_LEN bytes&#125; *&#x2F;\n    if (lseek(fd, C_START_OFF, SEEK_SET) &lt; 0)\n        FATAL(&quot;lseek C failed\\n&quot;);\n    if (write(fd, gbufC, C_LEN) &lt; 0)\n        FATAL(&quot;write C failed\\n&quot;);\n    return 0;\n&#125;\n\n请注意，我们如何连续三次编写代码来使用一对系统调用&#123;lseek, write&#125;，让我们来尝试一下：\n$ .&#x2F;sgio_simple \nUsage: .&#x2F;sgio_simple use-method-option\n 0 &#x3D; traditional lseek&#x2F;write method\n 1 &#x3D; better SG IO method\n$ .&#x2F;sgio_simple 0\nIn setup_buffers_goto()\nIn wr_discontig_the_normal_way()\n$ ls -l tmptest \n-rw-rw-r--. 1 kai kai 222 Oct 16 08:45 tmptest\n$ hexdump -x tmptest \n0000000 0000 0000 0000 0000 0000 4141 4141 4141\n0000010 4141 4141 4141 4141 4141 4141 4141 0000\n0000020 0000 0000 0000 0000 0000 0000 0000 0000\n*\n0000080 0000 4242 4242 4242 4242 4242 4242 4242\n0000090 4242 4242 4242 4242 4242 4242 4242 4242\n00000a0 0000 0000 0000 0000 0000 0000 0000 0000\n00000b0 0000 0000 4343 4343 4343 4343 4343 4343\n00000c0 4343 4343 4343 4343 4343 4343 4343 4343\n00000d0 4343 4343 4343 4343 4343 4343 4343 \n00000de\n\n它起作用了；我们创建的文件tmptest的长度为222字节，尽管实际数据内容(AS、BS和Cs)的长度为20+30+42&#x3D;92字节。剩下的(222-92)130个字节是文件中的三个洞。hexdump实用程序可以方便地转储文件的内容；0x41表示A，0x42表示B，0x43表示C。可以清楚地看到空洞是我们想要的长度的空填充区域。\n不连续数据文件-SG-I&#x2F;O方法使用连续三次的系统调用对的传统方法当然有效，但性能损失相当大；事实上，发出系统调用被认为是非常昂贵的。在性能方面，一种更优越的方法称为分散聚集I&#x2F;O(SG-I&#x2F;O，或向量化I&#x2F;O)。相关的系统调用是readv和writev；这是它们的签名：\n#include &lt;sys&#x2F;uio.h&gt;\nssize_t readv(int fd, const struct iovec *iov, int iovcnt);\nssize_t writev(int fd, const struct iovec *iov, int iovcnt);\n\n这些系统调用允许您指定一组段以一次读取或写入；每个段通过名为iovec的结构描述单个I&#x2F;O操作：\nstruct iovec &#123;\n    void *iov_base; &#x2F;* Starting address *&#x2F;\n    size_t iov_len; &#x2F;* Number of bytes to transfer *&#x2F;\n&#125;;\n\n程序员可以传递描述要执行的I&#x2F;O操作的段数组；这正是第二个参数-指向struct iovecs数组的指针；第三个参数是要处理的段数。第一个参数很明显–文件描述符，表示要对其执行集中读取或分散写入的文件。\n因此，想想看：您可以将来自给定文件的不连续读取聚集到您通过I&#x2F;O向量指针指定的缓冲区(及其大小)中，并且可以从您通过I&#x2F;O向量指针指定的缓冲区(及其大小)分散对给定文件的不连续写入；因此，这些类型的多个不连续I&#x2F;O操作被称为scatter-gatherI&#x2F;O！这是真正酷的部分：系统调用保证以数组顺序和原子方式执行这些I&#x2F;O操作；也就是说，只有当所有操作完成时，它们才会返回。不过，还是要小心：从readv或writev返回的值是实际读取或写入的字节数，如果失败，返回值为-1。I&#x2F;O操作执行的数量总是可能少于请求的数量；这不是故障，应该由开发人员进行检查。\n现在，对于前面的数据文件示例，让我们看一下通过writev设置和执行不连续的分散有序原子写入的代码：\nstatic int wr_discontig_the_better_SGIO_way(int fd)\n&#123;\n  struct iovec iov[6];\n  int i&#x3D;0;\n\n  &#x2F;* We don&#39;t want to call lseek of course; so we emulate the seek\n   * by introducing segments that are just &quot;holes&quot; in the file. *&#x2F;\n\n  &#x2F;* A: &#123;seek_to A_START_OFF, write gbufA for A_LEN bytes&#125; *&#x2F;\n  iov[i].iov_base &#x3D; gbuf_hole;\n  iov[i].iov_len &#x3D; A_HOLE_LEN;\n  i ++;\n  iov[i].iov_base &#x3D; gbufA;\n  iov[i].iov_len &#x3D; A_LEN;\n\n  &#x2F;* B: &#123;seek_to B_START_OFF, write gbufB for B_LEN bytes&#125; *&#x2F;\n  i ++;\n  iov[i].iov_base &#x3D; gbuf_hole;\n  iov[i].iov_len &#x3D; B_HOLE_LEN;\n  i ++;\n  iov[i].iov_base &#x3D; gbufB;\n  iov[i].iov_len &#x3D; B_LEN;\n\n  &#x2F;* C: &#123;seek_to C_START_OFF, write gbufC for C_LEN bytes&#125; *&#x2F;\n  i ++;\n  iov[i].iov_base &#x3D; gbuf_hole;\n  iov[i].iov_len &#x3D; C_HOLE_LEN;\n  i ++;\n  iov[i].iov_base &#x3D; gbufC;\n  iov[i].iov_len &#x3D; C_LEN;\n  i ++;\n\n  &#x2F;* Perform all six discontiguous writes in order and atomically! *&#x2F;\n  if (writev(fd, iov, i) &lt; 0)\n    return -1;\n&#x2F;* Do note! As mentioned in Ch 19:\n   * &quot;the return value from readv(2) or writev(2) is the actual number\n   * of bytes read or written, and -1 on failure. It&#39;s always possible\n   * that an I&#x2F;O operation performs less than the amount requested; this\n   * is not a failure, and it&#39;s up to the developer to check.&quot;\n   * Above, we have _not_ checked; we leave it as an exercise to the\n   * interested reader to modify this code to check for and read&#x2F;write\n   * any remaining bytes (similar to this example: ch7&#x2F;simpcp2.c).\n   *&#x2F;\n  return 0;\n&#125;\n\n最终结果与传统方法相同；我们将其留给读者来尝试和查看。这是关键点：传统方法要求我们至少发出六个系统调用(3个&#123;lSeek，WRITE&#125;对)来执行不连续数据写入文件，而SG-I&#x2F;O代码只使用一个系统调用执行完全相同的不连续数据写入。这会带来显著的性能提升，尤其是对于I&#x2F;O工作负载繁重的应用程序。\nSG-I&#x2F;O变化回想一下在MT应用程序文件I&#x2F;O中使用pread，pWRITE API一节，我们可以使用prew(2)和pwrite(2)系统调用通过多线程(在多线程应用程序中)有效地并行执行文件I&#x2F;O。类似地，Linux提供了preadv和pwritev系统调用；正如您可以猜到的那样，它们通过添加第四个参数偏移量提供了readv和writev的功能；就像使用readv和writev一样，可以指定要执行SG-IO的文件偏移量，并且不会更改它(同样，对于MT应用程序可能很有用)。preadv和pwritev的签名如下所示：\n#include &lt;sys&#x2F;uio.h&gt;\nssize_t preadv(int fd, const struct iovec *iov, int iovcnt,\n                      off_t offset);\nssize_t pwritev(int fd, const struct iovec *iov, int iovcnt,\n                       off_t offset);\n\n最新的Linux内核(有些是4.6版以上)还提供了API的进一步变体：preadv2和pwritev2系统调用。与以前的API的不同之处在于，它们采用额外的第五个参数标志，允许开发人员通过能够指定它们是同步(通过RWF_DSYNC和RWF_SYNC标志)、高优先级(通过RWF_HIPRI标志)还是非阻塞(通过RWF_NOWIT标志)来更好地控制SG-I&#x2F;O操作的行为。有关详细信息，请读者参阅preadv2&#x2F;pwritev上的手册页。\n通过内存映射的I&#x2F;O文件在本章中，我们都多次提到Linux内核的PageCache如何通过缓存其中的文件内容来帮助极大地提高性能(减少了每次访问非常慢的存储设备而只读或写RAM中的数据块的需要)。然而，尽管我们通过页面缓存获得了性能，但同时使用传统的read write api 和甚至更快的pread   pwrite readv writev preadv pwritev api,但是仍然存在1个隐藏的问题。\nLinux I&#x2F;O代码路径简介要了解问题所在，我们首先必须更深入地了解I&#x2F;O代码路径的实际工作方式；下图概括了相关要点：\n\n\n\n\n假设进程P1打算从它已打开的目标文件中读取大约12KB的数据(通过open系统调用)；我们设想它将通过通常的方式完成此操作：\n\n通过mallocAPI分配12KB的堆缓冲区(3页&#x3D;12,288字节)。\n发出read系统调用，将数据从文件读入堆缓冲区。\nread系统调用在操作系统中执行工作；当读取完成时，它返回(希望返回值12,288；记住，程序员的工作是检查这一点，并且不做任何假设)。\n\n\n\n这听起来很简单，但在幕后还有更多的事情发生，深入挖掘一下符合我们的利益。下面是所发生情况的更详细的视图(在上图中，数字点1、2和3在一个圆圈中显示；下面是)：\n\n进程P1通过mallocAPI(len&#x3D;12KB&#x3D;12,288字节)分配12KB的堆缓冲区。\n接下来，它发出一个read系统调用，将数据从文件(由fd指定)读取到刚刚分配的堆缓冲区buf中，长度为12KB。\n因为read是一个系统调用，所以进程(或线程)现在切换到内核模式(还记得我们在第1章，Linux系统体系结构中介绍的单片设计吗)；它进入Linux内核的通用文件系统层(称为虚拟文件系统交换机(VFS))，在那里它将被自动分流到其适当的底层文件系统驱动程序(可能是ext4文件系统驱动程序)上，之后Linux内核将首先检查：所需文件数据的这些页面是否已缓存在我们的页面缓存中？如果是，则作业已完成(我们跳到步骤7)，只需将页面复制回用户空间缓冲区。假设我们得到一个缓存未命中–所需的文件数据页不在页面缓存中。\n因此，内核首先为页面缓存分配足够的RAM(页帧)(在我们的示例中，页面缓存内存区域中显示为粉红色正方形的三个帧)。然后，它向请求文件数据的底层发出适当的I&#x2F;O请求。\n请求最终到达块(存储)驱动程序；我们假设它知道自己的工作，并从底层存储设备控制器(可能是磁盘或闪存控制器芯片)读取所需的数据块。然后(有趣的是)它被赋予一个目标地址来写入文件数据；它是页面缓存中分配的页帧的地址(步骤4)；因此，块驱动程序总是将文件数据写入内核的页面缓存，而不是直接返回到用户模式进程缓冲区。\n块驱动程序已成功地将数据块从存储设备(或其他设备)复制到内核页面缓存中先前分配的帧中。(实际上，这些数据传输通过一种称为直接内存访问(DMA)的高级内存传输技术进行了高度优化，在这种技术中，驱动程序本质上利用硬件在设备和系统内存之间直接传输数据，而无需CPU干预。显然，这些主题远远超出了本书的范围。)\n现在，内核将刚刚填充的内核页面缓存帧复制到用户空间堆缓冲区中。\n(阻塞的)read系统调用现在终止，返回值12,288，表示所有三页文件数据确实已被传输(同样，作为应用程序开发人员，您应该检查该返回值并不做任何假设)。\n\n一切看起来都很棒，对吧？其实并非如此；仔细考虑一下这一点：尽管readAPI确实成功了，但这一成功是付出了相当大的代价的：内核必须分配RAM(页帧)以在其页缓存中保存文件数据(步骤4)，一旦数据传输完成(步骤6)，然后将内容复制到用户空间堆内存(步骤7)。因此，通过保留额外的数据副本，我们使用了两倍于应有的内存量。这是非常浪费的，显然，数据缓冲区在块驱动程序到内核页面缓存，然后内核页面缓存到用户空间堆缓冲区之间的多次复制也会降低性能(更不用说CPU缓存会不必要地处理所有这些垃圾内容)。使用前面的代码模式，不等待速度较慢的存储设备的问题得到了解决(通过页面缓存效率)，但其他方面都很差-我们实际上已经将所需的内存使用量增加了一倍，并且在进行复制时，CPU缓存被(不必要的)文件数据覆盖。\n为I&#x2F;O映射文件的内存以下是这些问题的解决方案：通过mmap系统调用进行内存映射。Linux提供了非常强大的mmap系统调用；它使开发人员能够将任何内容直接映射到进程虚拟地址空间(VAS)。该内容包括文件数据、硬件设备(适配器)存储区域或仅通用存储区域。在本章中，我们将只关注使用mmap将常规文件的内容映射到进程VAS中。在讨论mmap如何成为我们刚才讨论的内存浪费问题的解决方案之前，我们首先需要更多地了解如何使用mmap系统调用本身。\nmmap系统调用的签名如下所示：\n#include &lt;sys&#x2F;mman.h&gt;\nvoid *mmap(void *addr, size_t length, int prot, int flags,\n           int fd, off_t offset);\n\n我们希望将文件的给定区域从给定的offset和length字节映射到我们的Process VAS中；下图描述了我们想要实现的简单视图：\n\n为了实现到进程VAS的文件映射，我们使用mmap系统调用。看一眼它的签名，很明显我们首先需要做的是：通过open打开要映射的文件(以适当的模式：只读或读写，取决于您想要做什么)，从而获得文件描述符；将该描述符作为第五个参数传递给mmap。要映射到进程VAS的文件区域可以分别通过第六个和第二个参数指定–映射应开始的文件偏移量和长度(以字节为单位)。\n第一个参数addr提示内核应该在进程VAS中的什么位置创建映射；建议在这里传递0(空)，允许操作系统决定新映射的位置。这是使用mmap的正确的便携方式；但是，有些应用程序(当然，还有一些恶意的安全黑客！)使用此参数可以尝试预测发生映射的位置。在任何情况下，在流程VAS中创建映射的实际(虚拟)地址是来自mmap的返回值；空返回值表示失败，必须进行检查。\n这里有一个确定映射位置的有趣技术：首先执行一个所需映射大小的malloc，并将返回值从这个malloc传递给mmap的第一个参数(还设置标志参数以包括MAP_FIXED位)！如果长度大于MMAP_THRESHOLD(默认情况下为128KB)并且大小是系统页面大小的倍数，则此方法可能会起作用。请再次注意，此技术不可移植，可能会也可能不会起作用。\n另一点需要注意的是，大多数映射(总是文件映射)都是按页面粒度执行的，即以页面大小的倍数执行；因此，返回地址通常是页面对齐的。\nmmap的第三个参数是一个整数位掩码prot–给定区域的内存保护(回想一下，我们已经在内存保护一节的第4章动态内存分配中遇到过内存保护)。Prot参数是位掩码，可以只是PROT_NONE位(表示没有权限)，也可以是余数的按位或；此表列举了位及其含义：\n\n\n\n保护位\n含义\n\n\n\nPROT_NONE\n不允许对页面进行访问\n\n\nPROT_READ\n页面上允许的读取数\n\n\nPROT_WRITE\n页面上允许的写\n\n\nPROT_EXEC\n执行页面上允许的访问权限\n\n\n当然，页面保护必须与文件的open相匹配。还要注意，在较旧的x86系统上，可写内存用于表示可读内存(即PROT_WRITE&#x3D;&gt;PROT_READ)。现在不再是这种情况了；您必须显式指定映射的页面是否可读(对于可执行页面也是如此：必须指定，文本段就是典型的例子)。为什么要使用PROT_NONE？保护页面就是一个实际的例子。\nmmap的优势现在我们已经了解了如何使用mmap系统调用，我们再回顾一下前面的讨论：回想一下，使用read/write会导致双重复制；内存浪费(加上CPU缓存也被丢弃的事实)。\n实现mmap如此有效地解决这个严重问题的关键在于：mmap通过在内部将包含文件数据(从存储设备读取)的内核页缓存页直接映射到进程虚拟地址空间来建立文件映射。此图说明了这一点：\n\n\n\n\n映射不是副本；因此，基于mmap的文件I&#x2F;O被称为零复制技术：一种在I&#x2F;O缓冲区上执行工作的方法，其中只有一个副本由内核在其页缓存中维护；不需要更多的副本。\n事实是，设备驱动程序的作者希望使用零复制技术来优化他们的数据路径，mmap肯定是其中之一。\nmmap在设置映射(第一次)时确实会产生很大的开销，但一旦完成，I&#x2F;O就会非常快，因为它基本上是在内存中执行的。将mmap用于非常少量的I&#x2F;O工作可能并不是最优的；当指示大而持续的I&#x2F;O工作负载时，建议使用它。\n","slug":"高级文件IO","date":"2022-05-15T14:50:22.000Z","categories_index":"","tags_index":"linux,IO","author_index":"李志博的博客"},{"id":"7a8ccfb903ca33a7195e4b4486061453","title":"RocketMQ-NameServer的源码阅读脉络","content":"启动大概流程\nNamesrvStartup 是启动类，会先调createNamesrvController 初始化配置，然后创建NamesrvController, 然后调Controller的静态start方法\nNamesrvController的静态Start方法会先调initialize方法，然后注册程序destory的钩子，最后start方法启动\n在initialize 方法里会执行registerProcessor方法，会在这里注册一个NettyRequestProcessor 这个是1个接口，表示的是RocketMQ请求处理，在NameServer里，有2个实现，ClusterTestRequestProcessor 和 DefaultRequestProcessor其中我们主要关键的是DefaultRequestProcessor 。\n\n\n\n处理请求路由DefaultRequestProcessor是NameServer里我觉得最核心的类，因为它是NameServer里负责接收并处理请求的类，通过它的processRequest方法，可以很直接的看到NameServer都具体负责处理哪些请求，以及这些请求具体的实现，也都可以找的到。\n核心类介绍\nKVConfigManager 负责管理不同Namespace下KV的配置，并存储。\nRouteInfoManager 负责管理路由相关信息\nNamesrvController 是1个集中的初始化整个系统的类，并且持有其他核心类的依赖，其他类核心类都持有NamesrvController的引用，要跟另外个类调用，都是通过NamesrvController来获取这个类的引用，相当于1个解耦的作用。\n\n","slug":"RocketMQ-NameServer的源码阅读脉络","date":"2022-05-15T04:00:02.000Z","categories_index":"","tags_index":"RocketMQ","author_index":"李志博的博客"},{"id":"957c5556bfb48cc069f3d5abcb598019","title":"Maven多模块统一管理版本","content":"之前写多模块项目的时候，一般都需要保证各模块的版本号是一致的，所以导致每次代码有变动需要升级版本的时候，都需要全局挨个替换，但是这种方式太麻烦，也不够优雅，由于最近工作是维护公司的apollo，所以我看了下apollo的pom文件里的方式，觉得还不错。\n首先是要在多模块项目里的父pom上，做如下配置\n&lt;groupId&gt;com.example&lt;&#x2F;groupId&gt;\n&lt;artifactId&gt;demo-mulitmodule-release-plugin&lt;&#x2F;artifactId&gt;\n&lt;version&gt;$&#123;reversion&#125;&lt;&#x2F;version&gt;\n&lt;packaging&gt;pom&lt;&#x2F;packaging&gt;\n&lt;properties&gt;\n\t&lt;reversion&gt;0.0.1-SNAPSHOT&lt;&#x2F;reversion&gt;\n&lt;&#x2F;properties&gt;\n\n为了精简，以上我只贴了相关的配置，packaging 是写死的，必须是pom,version要通过properties设置属性引用的方式，这样方便在后续子pom里通过同样的方式去引用，这样后续版本变化只改这里就可以了。groupId和artifactId 写自己的就行，这里只是为了合后面子pom的配置保持上下文一致。\n然后子pom的相关配置如下:\n&lt;parent&gt;\n\t&lt;groupId&gt;com.example&lt;&#x2F;groupId&gt;\n\t&lt;artifactId&gt;demo-mulitmodule-release-plugin&lt;&#x2F;artifactId&gt;\n\t&lt;version&gt;$&#123;reversion&#125;&lt;&#x2F;version&gt;\n\t&lt;relativePath&gt;..&#x2F;pom.xml&lt;&#x2F;relativePath&gt;\n&lt;&#x2F;parent&gt;\n\n上面配置就是明确子pom继承了前面的父pom，并通过$&#123;reversion&#125;实现父pom版本号的引用和relativePath的设置来定位父pom文件的位置。\n","slug":"Maven多模块统一管理版本","date":"2022-05-14T05:27:00.000Z","categories_index":"","tags_index":"maven","author_index":"李志博的博客"},{"id":"472d9f1a708133ccb720a7af7f9b0d65","title":"MySQL查询Value区分大小写","content":"今天1个同事问我1个问题，MySQL查询如何区分大小写，当时我就惊了，难道默认不区分吗？于是我做了一下尝试，在本地建了1个表，发现还真的不区分，效果如下: \n\n\n然后我执行下面的SQL发现居然可以把zhangsan 查的到\nselect * from student where name &#x3D; &#39;Zhangsan&#39;\n\n其中Z是我故意大写的，结果如下:\n\n\n要想解决这个问题，可以通过binary关键字解决，加在列的前面\nselect * from student where binary name &#x3D; &#39;Zhangsan&#39;\n\n这样就查不到了\n\n\n\n\n","slug":"MySQL查询Value区分大小写","date":"2022-05-14T02:23:27.000Z","categories_index":"","tags_index":"mysql","author_index":"李志博的博客"}]