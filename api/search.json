[{"id":"9f14b7d15ba15e4a4bdd3158410a83f4","title":"Mockito跟SpringBoot@Autowired组合使用方式","content":"有的时候，我们想绝大多数Bean在单测的时候，还是希望走正常的Bean注入,只有少部分Bean需要自己人工Mock结果，这个时候如何优雅的Mock Bean实例并自动和其他Bean实例一样进行注入呢？\n可以通过Mockito的@MockBean注解解决，代码如下\n@SpringBootTest\nclass CubeDataCacheServiceTest &#123;\n\n    @Autowired\n    private CubeDataCacheService cubeDataCacheService;\n\n    @MockBean\n    private DimensionLocalService dimensionLocalService;\n&#125;\n\n以上代码把一些不相关的代码都去掉了，接着我们就可以正常的调mockito的API，进行正常的设置mock结果就好了。\n","slug":"Mockito跟SpringBoot-Autowired组合使用方式","date":"2022-06-04T06:10:53.000Z","categories_index":"","tags_index":"mockito,SpringBoot","author_index":"李志博的博客"},{"id":"e939d2a67c854d9584266b38aea34ab3","title":"Eclipse-Collection之IntIntHashMap","content":"最近在设计1个功能，其中需要内存保存数据的全量索引，结构大概如下:\nMap&lt;Integer,Integer&gt; indexMap &#x3D; new HashMap&lt;&gt;();\n\n我跑单测观察这个Map占用内存的时候发现，110多万的数据要消耗500多MB内存，这个其实对内存的占用已经算是非常大了，那么就可以用标题说的Eclipse-Collection库的IntIntHashMap来替代之前的HashMap\n首先我们引包:\n&lt;dependency&gt;\n        &lt;groupId&gt;org.eclipse.collections&lt;&#x2F;groupId&gt;\n        &lt;artifactId&gt;eclipse-collections-api&lt;&#x2F;artifactId&gt;\n        &lt;version&gt;11.0.0&lt;&#x2F;version&gt;\n    &lt;&#x2F;dependency&gt;\n\n    &lt;dependency&gt;\n        &lt;groupId&gt;org.eclipse.collections&lt;&#x2F;groupId&gt;\n        &lt;artifactId&gt;eclipse-collections&lt;&#x2F;artifactId&gt;\n        &lt;version&gt;11.0.0&lt;&#x2F;version&gt;\n    &lt;&#x2F;dependency&gt;\n\n然后IntIntHashMap的使用和普通Map区别不大，只不过由于确定了Key和Value都是Int，所以就不需要泛型了\n@Test\nvoid printObjectSize2() throws InterruptedException &#123;\n    Thread.sleep(1000 * 60);\n    System.out.println(&quot;begin&quot;);\n    IntIntHashMap intHashMap &#x3D; new IntIntHashMap();\n    for (int i &#x3D; 0; i &lt; 1000000; i++) &#123;\n        intHashMap.put(i, i);\n    &#125;\n    System.out.println(&quot;end&quot;);\n    Thread.currentThread().join();\n&#125;\n\n执行上面的单测，begin前面的sleep是为了预留登录arthas的时间，然后通过执行memory观察当前堆内存大小。\n\n然后执行到end的时候，在执行一次memory看下堆的大小，发现这次只占用61MB。\n\n","slug":"Eclipse-Collection之IntIntHashMap","date":"2022-05-23T14:09:07.000Z","categories_index":"","tags_index":"Java,Eclipse-Collection","author_index":"李志博的博客"},{"id":"ece203b53ac6e642a41832310009d6ab","title":"翻译bpf-performance-tools第7章内存","content":"Linux是一个基于虚拟内存的系统，其中每个进程都有自己的虚拟地址空间，并按需映射到物理内存。它的设计允许超额订阅物理内存，Linux通过一个页出守护进程和物理交换设备以及(作为最后手段)内存不足(OOM)杀手来管理物理内存。Linux使用空闲内存作为文件系统缓存，这一主题将在第8章中介绍。\n本章展示了BPF如何以新的方式公开应用程序内存使用情况，并帮助您检查内核如何应对内存压力。随着CPU可伸缩性的增长快于内存速度，内存I&#x2F;O已成为新的瓶颈。了解内存使用情况可以获得许多性能优势。\n学习目标：\n\n了解内存分配和分页行为\n了解使用跟踪器成功分析内存行为的策略\n使用传统工具了解内存容量使用情况\n使用BPF工具识别导致堆和RSS增长的代码路径\n通过文件名和堆栈跟踪来表征页面错误\n分析VM扫描程序的行为\n确定内存回收对性能的影响\n确定哪些进程正在等待换入\n使用bpftrace一行程序以自定义方式探索内存使用情况\n\n本章从内存分析的一些必要背景开始，重点介绍应用程序的使用情况、总结虚拟和物理分配以及分页。探讨了BPF可以回答的问题，以及应遵循的总体战略。首先总结传统的内存分析工具，然后介绍BPF工具，包括BPF一行程序列表。本章以可选练习结束。\n7.1背景本节介绍内存基础知识、BPF功能以及建议的内存分析策略。\n7.1.1内存基础知识内存分配器图7-1显示了用户级和内核级软件常用的内存分配系统。对于使用libc进行内存分配的进程，内存存储在进程的虚拟地址空间的动态段中，称为堆。Libc提供内存分配函数，包括Malloc()和Free()。当内存被释放时，libc跟踪它的位置，并可以使用该位置信息来完成后续的Malloc()。只有在没有可用内存时，libc才需要扩展堆的大小。Libc通常没有理由缩小堆的大小，因为这都是虚拟内存，而不是真正的物理内存。\n\n图 7-1 内存分配\n","slug":"翻译bpf-performance-tools第7章内存","date":"2022-05-21T12:29:12.000Z","categories_index":"","tags_index":"linux,Memory,bpf","author_index":"李志博的博客"},{"id":"f929186e789271318e9dfdac1fb85b07","title":"native内存最佳实践","content":"堆是Java应用程序中最大的内存消耗者，但JVM将分配和使用大量本机内存。虽然第7章从编程的角度讨论了有效管理堆的方法，但堆的配置以及它如何与操作系统的本机内存交互是影响应用程序整体性能的另一个重要因素。这里存在术语冲突，因为C程序员倾向于将其本机内存的一部分称为C堆。为了与以Java为中心的世界观保持一致，我们将继续使用堆来引用Java堆，使用本机内存来引用JVM的非堆内存，包括C堆。\n本章讨论本机(或操作系统)内存的这些方面。我们首先讨论JVM的整个内存使用情况，目的是了解如何监视该使用情况以发现性能问题。然后，我们将讨论调优JVM和操作系统以优化内存使用的各种方法。\n占用空间堆(通常)占JVM使用的最大内存量，但JVM也使用内存进行其内部操作。这种非堆内存是本机内存。本机内存也可以在应用程序中分配(通过对Malloc()和类似方法的JNI调用，或者在使用新I&#x2F;O或NIO时)。JVM使用的本机内存和堆内存的总和产生了应用程序的总占用空间。\n从操作系统的角度来看，这一总占用空间是性能的关键。如果没有足够的物理内存来容纳应用程序的全部占用空间，则性能可能会开始下降。这里的关键字是可能。本机内存的一部分仅在启动期间使用(例如，与在类路径中加载JAR文件相关联的内存)，如果该内存被换出，则不一定会被注意到。一个Java进程使用的一些本机内存与系统上的其他Java进程共享，一些较小的部分与系统上的其他类型的进程共享。不过，在大多数情况下，为了获得最佳性能，您希望确保所有Java进程的总内存占用不超过机器的物理内存(另外，您还希望为其他应用程序留出一些内存)。\n测量占用空间要测量进程的总内存使用量，您需要使用特定于操作系统的工具。在基于Unix的系统中，top和ps等程序可以显示基本级别的数据；在Windows系统中，您可以使用perfmon或VMMap。无论使用哪种工具和平台，都需要查看进程的实际分配内存(而不是驻留内存)。\n分配内存和保留内存之间的区别取决于JVM(和所有程序)管理内存的方式。考虑使用参数-Xms512m  -Xmx2048m指定的堆。堆从使用512MB开始，它将根据需要调整大小，以满足应用程序的GC目标。\n这一概念是已提交(或已分配)内存和驻留内存(有时称为进程的虚拟大小)之间的本质区别。JVM必须告诉操作系统它可能需要多达2 GB的内存用于堆，以便驻留内存：操作系统承诺，当JVM增加堆的大小时尝试分配额外的内存时，该内存将是可用的。\n尽管如此，最初只分配了512MB内存，这512MB是(堆)正在使用的所有内存。该(实际分配的)内存称为提交内存。提交的内存量将随着堆大小的调整而波动；具体地说，随着堆大小的增加，提交的内存量也相应增加。\n超额预定是个问题吗？\n当我们查看性能时，只有提交的内存才是真正重要的：性能问题从来不是由于保留太多内存而导致的。\n但是，有时您希望确保JVM不会保留太多内存。对于32位JVM来说尤其如此。由于32位应用程序的最大进程大小为4 GB(或更小，具体取决于操作系统)，因此过度保留内存可能是个问题。为堆保留3.5 GB内存的JVM只剩下0.5 GB的本机内存用于堆栈、代码缓存等。如果堆扩展到只提交1 GB的内存，这并不重要：由于3.5 GB的保留，用于其他操作的内存量被限制为0.5 GB。\n64位JVM不受进程大小的限制，但受机器上的虚拟内存总量的限制。假设您有一台具有4 GB物理内存和10 GB虚拟内存的小型服务器，并启动一个最大堆大小为6 GB的JVM。这将保留6 GB的虚拟内存(外加更多的非堆内存段)。无论堆变得多大(以及提交的内存有多大)，这个JVM只能在该机器上保留不到4 GB的内存。\n在所有条件相同的情况下，增加JVM结构的大小并让JVM以最佳方式使用该内存是很方便的。但这并不总是可行的。\n这种差异几乎适用于JVM分配的所有重要内存。随着更多代码的编译，代码缓存从初始值增长到最大值。元空间与堆分开分配，并在其初始(提交)大小和最大(保留)大小之间增长。\n线程堆栈是一个例外。每次JVM创建线程时，操作系统都会分配一些本机内存来保存该线程的堆栈，从而将更多内存分配给进程(至少在线程退出之前)。不过，线程堆栈在创建时是完全分配的。\n在Unix系统中，应用程序的占用空间可以通过各种操作系统工具报告的进程的驻留集大小(RSS)来估计。该值很好地估计了一个进程正在使用的已提交内存量，尽管它在两个方面并不准确。首先，JVM和其他进程之间在操作系统级别共享的少数几个页面(即共享库的文本部分)将计入每个进程的RSS。其次，一个进程在任何时刻提交的内存都可能比它调入的内存多。不过，跟踪进程的RSS是监视总内存使用的一种很好的首选方法。在较新的Linux内核上，PSS是RSS的改进，它删除了与其他程序共享的数据。\n在Windows系统上，等同的概念称为应用程序的工作集，这是任务管理器报告的内容。\n最大限度地减少占用空间要最大限度地减少JVM使用的内存使用量，请限制以下各项使用的内存量：\n\n堆\n堆是最大的内存块，尽管令人惊讶的是它可能只占用总内存的50%到60%。使用较小的最大堆(或设置GC调优参数以使堆永远不会完全扩展)会限制程序的内存占用。\n\n\n\n线程堆栈\n线程堆栈非常大，特别是对于64位JVM而言。\n\n\n\n代码缓存\n代码缓存使用本机内存来保存编译后的代码。正如在第4章中所讨论的，这是可以调优的(尽管如果由于空间限制而无法编译所有代码，则性能将受到影响)。\n\n\n\n本机库分配\n本机库可以分配它们自己的内存，这有时可能非常重要。\n\n\n接下来的几节将讨论如何监控和减少这些区域。\n快速总结\n\nJVM的总内存占用对其性能有很大影响，特别是在机器上的物理内存受限的情况下。内存占用是性能测试的另一个应该通常监视的方面。\n\n\n本机内存跟踪JVM提供了有关它如何分配本机内存的有限可见性。重要的是要认识到，这种跟踪适用于由代码JVM本身分配的内存，但不包括由应用程序使用的本机库分配的任何内存。这既包括第三方本机库，也包括JDK本身附带的本机库(例如，libsocket.so)。\n使用选项-XX:NativeMemoryTracking=*off|summary|detail可以实现这种可见性。默认情况下，本机内存跟踪(NMT)处于关闭状态。如果开启了汇总或明细模式，您可以随时从jcmd获取本机内存信息：\n如果使用参数-XX:+PrintNMTStatistics(默认情况下为FALSE)启动JVM，则当程序退出时，JVM将输出有关分配的信息。\n以下是一个JVM的汇总输出，该JVM的初始堆大小为512 MB，最大堆大小为4 GB：\nNative Memory Tracking:\n\n Total: reserved&#x3D;5947420KB, committed&#x3D;620432KB\n\n尽管JVM总共预留了5.9 GB的内存，但它使用的内存要少得多：只有620MB。这是相当典型的(这也是不特别注意操作系统工具中显示的进程的虚拟大小的原因之一，因为这只反映了内存预留)。\n这种内存使用情况如下所示。堆本身(毫不奇怪)是驻留内存中最大的部分，为4 GB。但是堆的动态大小意味着它只增长到268 MB(在本例中，堆大小是-Xms256m-Xmx4g，因此实际的堆使用量只有少量扩展)：\n-                 Java Heap (reserved&#x3D;4194304KB, committed&#x3D;268288KB)\n                            (mmap: reserved&#x3D;4194304KB, committed&#x3D;268288KB)\n\n接下来是用于保存类元数据的本机内存。同样，请注意，JVM保留的内存比用来容纳程序中的24,316个类的内存要多。此处的提交大小将从MetaspaceSize标志的值开始，并根据需要增长，直到达到MaxMetaspaceSize标志的值：\n-                     Class (reserved&#x3D;1182305KB, committed&#x3D;150497KB)\n                            (classes #24316)\n                            (malloc&#x3D;2657KB #35368)\n                            (mmap: reserved&#x3D;1179648KB, committed&#x3D;147840KB)\n\n分配了77个线程堆栈，每个线程堆栈大约1 MB：\n-                    Thread (reserved&#x3D;84455KB, committed&#x3D;84455KB)\n                            (thread #77)\n                            (stack: reserved&#x3D;79156KB, committed&#x3D;79156KB)\n                            (malloc&#x3D;243KB, #314)\n                            (arena&#x3D;5056KB, #154)\n\n然后是JIT代码缓存：24,316个类不是很多，所以只提交了一小部分代码缓存：\n-                      Code (reserved&#x3D;102581KB, committed&#x3D;15221KB)\n                            (malloc&#x3D;2741KB, #4520)\n                            (mmap: reserved&#x3D;99840KB, committed&#x3D;12480KB)\n\n接下来是GC算法用于处理的堆之外的区域。此区域的大小取决于正在使用的GC算法：(简单的)串行收集器保留的空间将远远少于更复杂的G1GC算法(尽管一般来说，这里的保留量永远不会很大)：\n-                        GC (reserved&#x3D;199509KB, committed&#x3D;53817KB)\n                            (malloc&#x3D;11093KB #18170)\n                            (mmap: reserved&#x3D;188416KB, committed&#x3D;42724KB)\n\n类似地，除了放置在代码缓存中的结果代码外，编译器还使用该区域进行其操作：\n-                  Compiler (reserved&#x3D;162KB, committed&#x3D;162KB)\n                            (malloc&#x3D;63KB, #229)\n                            (arena&#x3D;99KB, #3)\n\nJVM的内部操作在这里表示。它们中的大多数往往都很小，但有一个重要的例外是direct byte buffers，其分配如下：\n-                  Internal (reserved&#x3D;10584KB, committed&#x3D;10584KB)\n                            (malloc&#x3D;10552KB #32851)\n                            (mmap: reserved&#x3D;32KB, committed&#x3D;32KB)\n\n符号表引用(来自类文件的常量)保存在这里：\n-                    Symbol (reserved&#x3D;12093KB, committed&#x3D;12093KB)\n                            (malloc&#x3D;10039KB, #110773)\n                            (arena&#x3D;2054KB, #1)\n\nNMT本身需要一些空间来运行(这是默认情况下不启用它的原因之一)：\n-    Native Memory Tracking (reserved&#x3D;7195KB, committed&#x3D;7195KB)\n                            (malloc&#x3D;16KB #199)\n                            (tracking overhead&#x3D;7179KB)\n\n最后，以下是JVM的一些次要记账部分：\n-               Arena Chunk (reserved&#x3D;188KB, committed&#x3D;188KB)\n                            (malloc&#x3D;188KB)\n-                   Unknown (reserved&#x3D;8192KB, committed&#x3D;0KB)\n                            (mmap: reserved&#x3D;8192KB, committed&#x3D;0KB)\n\n\n详细的内存跟踪信息\n如果JVM以-XX:NativeMemoryTracking=detail启动，那么jcmd(带有最后一个detail参数)将提供有关本机内存分配的详细信息。这包括整个内存空间的映射，其中包括如下行：\n0x00000006c0000000 - 0x00000007c0000000] reserved 4194304KB for Java Heap\n        from [ReservedSpace::initialize(unsigned long, unsigned long,\n                            bool, char*, unsigned long, bool)+0xc2]\n        [0x00000006c0000000 - 0x00000006fb100000] committed 967680KB\n            from [PSVirtualSpace::expand_by(unsigned long)+0x53]\n        [0x000000076ab00000 - 0x00000007c0000000] committed 1397760KB\n            from [PSVirtualSpace::expand_by(unsigned long)+0x53]\n\n这4 GB的堆空间是在初始化()函数中保留的，其中两次分配是在Expand_by()函数中进行的。\n这种信息会在整个进程空间中重复出现。如果您是JVM工程师，它提供了一些有趣的线索，但对于我们其他人来说，摘要信息足够有用了。\n总体而言，NMT提供了两条关键信息：\n\n提交的总大小\nJVM的总提交大小(理想情况下)接近进程将消耗的物理内存量。反过来，这应该接近应用程序的RSS(或工作集)，但是那些由操作系统提供的度量不包括任何已提交但被调出进程的内存。事实上，如果进程的RSS小于提交的内存，这通常表示操作系统难以在物理内存中容纳所有JVM。\n\n\n\n个人提交大小\n当需要调优堆、代码缓存和元空间的最大值时，了解JVM使用了多少内存是很有帮助的。过度分配这些区域通常只会导致无害的内存保留，尽管当保留的内存很重要时，NMT可以帮助跟踪哪些地方可以削减这些最大大小。\n\n\n另一方面，正如我在本节开头所指出的，NMT不提供对共享库的本机内存使用的可见性，因此在某些情况下，总进程大小将大于JVM数据结构的提交大小。\nNMT随时间推移\nNMT还允许您跟踪一段时间内内存分配是如何发生的。在启用NMT的情况下启动JVM后，您可以使用以下命令建立内存使用基准：\njcmd process_id VM.native_memory baseline\n\n这会导致JVM标记其当前的内存分配。稍后，您可以将当前内存使用情况与该标记进行比较：\n% jcmd process_id VM.native_memory summary.diff\nNative Memory Tracking:\n\nTotal:  reserved&#x3D;5896078KB  -3655KB, committed&#x3D;2358357KB -448047KB\n\n-             Java Heap (reserved&#x3D;4194304KB, committed&#x3D;1920512KB -444927KB)\n                        (mmap: reserved&#x3D;4194304KB, committed&#x3D;1920512KB -444927KB)\n\n在本例中，JVM保留了5.8 GB的内存，目前正在使用2.3 GB。承诺的大小比建立基线时减少了448 MB。类似地，堆使用的提交内存减少了444MB(可以检查输出的其余部分，以查看内存使用量减少的其他地方占剩余的4MB)。\n这是一种检查JVM随时间变化的内存占用情况的有用技术。\n\nNMT自动禁用\n在NMT输出中，我们看到NMT本身需要本机内存。此外，启用NMT将创建帮助进行内存跟踪的后台线程。\n如果JVM的内存或CPU资源变得非常紧张，NMT将自动关闭以节省资源。这通常是一件好事–除非你需要诊断的是紧张的情况。在这种情况下，您可以通过禁用   -XX:-AutoShutdownNMT NMT标志(默认情况下为true)来确保NMT继续运行。\n\n快速总结\n\n本机内存跟踪(NMT)提供有关JVM本机内存使用的详细信息。从操作系统的角度来看，这包括JVM堆(对于操作系统来说，它只是本机内存的一部分)。\nNMT的摘要模式足以进行大多数分析，并允许您确定JVM已经提交了多少内存(以及该内存用于什么)。\n\n\n共享库本机内存从体系结构的角度来看，NMT是HotSpot的一部分：运行应用程序的Java字节码的C++引擎。这是在JDK本身之下的，因此它不在JDK级别跟踪任何东西的分配。这些分配来自共享库(由System.loadLibrary()调用加载的库)。\n共享库通常被认为是Java的第三方扩展：例如，Oracle WebLogic Server有几个本机库，它使用它们比JDK更有效地处理I&#x2F;O。但JDK本身有几个本机库，与所有共享库一样，这些库都不在NMT的视野中。\n因此，NMT通常不会检测到本机内存泄漏–即应用程序的RSS或工作集不断超时增长。NMT监控的内存池通常都有一个上限(例如，最大堆大小)。NMT在告诉我们哪些池正在使用大量内存(因此哪些需要调优以使用更少的内存)方面很有用，但是不加绑定地泄漏本机内存的应用程序通常是因为本机库中的问题而这样做的。\n没有Java级别的工具可以真正帮助我们检测应用程序正在使用共享库中的本机内存的位置。操作系统级别的工具可以告诉我们，进程的工作集正在不断增长，如果进程增长到有10 GB的工作集，而NMT告诉我们JVM只分配了6 GB的内存，我们就知道其他4 GB的内存必须来自本地库分配。\n找出哪个本地库负责需要操作系统级别的工具，而不是来自JDK的工具。各种调试版本的malloc可用于此目的。这些在一定程度上是有用的，尽管本机内存通常是通过mmap调用来分配的，而大多数用于跟踪malloc调用的库都会错过这些。\n一个很好的替代方案是一个分析器，它可以分析本机代码和Java代码。例如，在第3章中，我们讨论了Oracle Studio Profiler，这是一个混合语言的Profiler。该分析器还具有跟踪内存分配的选项–需要注意的是，它只能跟踪本机代码的内存分配，而不能跟踪Java代码的内存分配，但这正是我们在本例中要考虑的问题。\n图8-1显示了Studio Profiler中的本机分配视图。\n\n这个调用图向我们展示了WebLogic本机函数mapFile已经使用mmap为我们的进程分配了大约150 GB的本机内存。这有点误导：存在到该文件的多个映射，而分析器不够智能，无法意识到它们共享的是实际内存：例如，如果15 GB文件有100个映射，则内存使用量仅增加15 GB。(坦率地说，我故意破坏了该文件，使其变得如此大；这根本不能反映实际使用情况。)尽管如此，本地分析器已经指出了问题的位置。\n在JDK本身内，有两个常见的操作可能会导致大量本机内存使用：使用Inflater和Deflater对象，以及使用NIO缓冲区。即使不进行性能分析，也有方法可以检测这些操作是否导致本机内存增长。\n本地内存和inflaters&#x2F;deflatersinflaters和deflaters执行各种压缩：ZIP、GZIP等。它们可以直接使用，也可以通过各种输入流隐式使用。这些不同的算法使用特定于平台的本地库来执行它们的操作。这些库可以分配大量的本机内存。\n当您使用这些类中的一个时，正如文档所述，您应该在操作完成时调用end()方法。除其他事项外，这还释放了对象使用的本机内存。如果您正在使用流，则应该关闭流(并且流类将在其内部对象上调用end()方法)。\n如果您忘记调用end()方法，也不会失去一切。回想一下第7章，所有对象都有一个完全针对这种情况的清理机制：在收集Inflater时，与对象相关联的finalize()方法(在JDK 8中)或Cleaner(在JDK 11中)可以调用end()方法。因此，您不会在这里泄漏本机内存；最终，对象将被收集并最终确定，本机内存将被释放。\n不过，这可能需要很长时间。Inflater的大小相对较小，并且在具有很少执行完整GC的大堆的应用程序中，这些对象很容易升级到老一代并停留几个小时。因此，即使在技术上没有泄漏-当应用程序执行完整的GC时，本机内存最终将被释放-在这里调用end()操作的失败可能具有所有本机内存泄漏的表现。\n在这个问题上，如果Inflater本身在Java代码中泄漏，那么本机内存实际上也会泄漏。\n因此，当大量本机内存泄漏时，获取应用程序的堆转储并查找这些Inflater和deflaters会很有帮助。这些对象很可能不会在堆本身中引起问题(它们太小了，不适合堆)，但它们中的大量对象将表明存在大量的本机内存使用。\n本地NIO缓冲如果NIO字节缓冲区是通过ByteBuffer类的allocateDirect()方法或FileChannel类的map()方法创建的，则它们将分配本机(堆外)内存。\n从性能角度来看，Native ByteBuffer很重要，因为它们允许本机代码和Java代码共享数据，而无需复制数据。用于文件系统和套接字操作的缓冲区是最常见的示例。将数据写入本机NIO缓冲区，然后将该数据发送到通道(例如，文件或套接字)，不需要在JVM和用于传输数据的C库之间复制数据。如果改为使用堆字节缓冲区，则必须由JVM复制缓冲区的内容。\nallocateDirect()方法调用的开销很大；应该尽可能多地重用直接字节缓冲区。理想的情况是，当线程是独立的，并且每个线程都可以保留一个直接的字节缓冲区作为线程局部变量时。如果许多线程需要不同大小的缓冲区，这有时会使用过多的本机内存，因为最终每个线程都会得到一个最大可能大小的缓冲区。对于这种情况–或者当线程本地缓冲区不适合应用程序设计时–直接字节缓冲区的对象池可能更有用。\n也可以通过对字节缓冲区进行切片(Slice)来管理它们。应用程序可以分配一个非常大的直接字节缓冲区，并且各个请求可以使用ByteBuffer类的Slice()方法从该缓冲区中分配一部分。当片的大小不总是相同时，此解决方案可能会变得笨拙：原始字节缓冲区可能会变成碎片，就像分配和释放不同大小的对象时堆会变成碎片一样。然而，与堆不同的是，字节缓冲区的各个片不能被压缩，因此只有当所有片的大小都一致时，该解决方案才能很好地工作。\n从调优的角度来看，使用这些编程模型要实现的一件事是，应用程序可以分配的直接字节缓冲区空间量可以受到JVM的限制。可分配给直接字节缓冲区的内存总量通过设置-XX:MaxDirectMemorySize=*N*标志来指定。在当前JVM中，此标志的缺省值为0。该限制的含义一直是频繁更改的主题，但在Java 8的更高版本(以及Java 11的所有版本)中，最大限制等于最大堆大小：如果最大堆大小为4 GB，您还可以在直接和&#x2F;或映射字节缓冲区中创建4 GB的堆外内存。如果需要，您可以将该值增加到超过最大堆值。\n分配给直接字节缓冲区的内存包括在NMT报告的内部部分；如果该数字很大，则几乎总是因为这些缓冲区。如果您想确切地知道缓冲区本身消耗了多少，mBeans会对此进行跟踪。检查mbean java.nio.BufferPool.direct.Attributes或java.nio.BufferPool.mappd.Attributes将显示每种类型已分配的内存量。图8-2显示了一个例子，其中我们映射了10个缓冲区，总计10MB的空间。\n\n由于内存分配库的设计，大型Linux系统有时会出现本机内存泄漏。这些库将本机内存划分为分配段，这有利于由多个线程进行分配(因为它限制了锁争用)。\n但是，本机内存的管理方式与Java堆不同：尤其是，本机内存永远不会被压缩。因此，本机内存中的分配模式可能会导致与第5章中描述的相同的碎片。\nJava中的本机内存可能会因为本机内存碎片而耗尽；这种情况最常发生在较大的系统上(例如，具有八个以上内核的系统)，因为Linux中的内存分区数是系统内核数的函数。\n有两件事可以帮助诊断这个问题：首先，应用程序将抛出一个OutOfMemoyError，说明它耗尽了本机内存。其次，如果您查看该进程的smaps文件，它将显示许多小的(通常为64 KB)分配。在这种情况下，补救方法是将环境变量MALLOC_ARENA_MAX设置为一个小数字，如2或4。该变量的默认值是系统上的内核数乘以8(这就是问题在大型系统上更常见的原因)。在这种情况下，本机内存仍将是碎片，但碎片应该不会那么严重。\n快速总结\n\n如果一个应用程序似乎使用了太多的本机内存，那么它很可能来自本机库，而不是JVM本身。\n本地配置文件可以有效地确定这些分配的来源。\n几个常见的JDK类通常会导致本机内存使用；请确保正确使用这些类。\n\n\n针对操作系统的JVM调优JVM可以使用多个调优来改进其使用操作系统内存的方式。\n大页关于内存分配和交换的讨论以页为单位进行。页面是操作系统用来管理物理内存的内存单位。它是操作系统的最小分配单位：当分配1个字节时，操作系统必须分配整个页面。对该程序的进一步分配来自同一页，直到它被填满，此时将分配新的页。\n操作系统分配的页面比物理内存所能容纳的页面多得多，这就是为什么有分页：地址空间的页面被移入和移出交换空间(或其他存储，取决于页面包含的内容)。这意味着这些页面和它们当前存储在计算机RAM中的位置之间必须有某种映射。这些映射有两种处理方式。所有页面映射都保存在全局页表中(操作系统可以扫描该表以找到特定的映射)，而最常用的映射保存在转换后备缓冲区(TLB)中。TLB保存在快速缓存中，因此通过TLB条目访问页面比通过页表访问要快得多。\n机器的TLB条目数量有限，因此最大限度地提高TLB条目的命中率变得很重要(它充当最近最少使用的缓存)。由于每个条目代表一页内存，因此增加应用程序使用的页大小通常是有利的。如果每个页面代表更多的内存，则需要更少的TLB条目来包含整个程序，并且更有可能在需要时在TLB中找到页面。一般来说，这对任何程序都是正确的，对于Java应用程序服务器或其他具有中等大小堆的Java程序也是如此。\n必须同时在Java和OS级别启用大页面。在Java级别，-XX:+UseLargePages标志启用大页面使用；默认情况下，该标志为FALSE。并不是所有的操作系统都支持大页面，启用它们的方式显然各不相同。并不是所有的操作系统都支持大页面，启用它们的方式显然各不相同。\n如果在不支持大页面的系统上启用了UseLargePages标志，则不会给出警告，并且JVM使用常规页面。如果在确实支持大页面的系统上启用了UseLargePages标志，但没有大页面可用(因为它们已全部在使用中或操作系统配置错误)，则JVM将打印一条警告。\nLinux巨型(大)页面Linux将大页面称为巨型页面。Linux上巨大页面的配置因版本而异；有关最准确的说明，请参考您的发行版的文档。但一般程序如下：\n\n确定内核支持哪些巨大的页面大小。该大小基于计算机处理器和内核启动时给定的引导参数，但最常见的值为2 MB：\n\n# grep Hugepagesize &#x2F;proc&#x2F;meminfo\nHugepagesize:       2048 kB\n\n\n计算出需要多少大页面。如果JVM将分配一个4 GB的堆，并且系统有2 MB的巨大页面，那么该堆将需要2,048个巨大的页面。可以使用的巨大页面的数量是在Linux内核中全局定义的，因此对将运行的所有JVM(以及将使用巨大页面的任何其他程序)重复此过程。您应该将这个值高估10%，以说明大型页面的其他非堆使用(因此，这里的示例使用2,200个大型页面)。\n\n将该值写入操作系统(以便立即生效)：\n\n\n# echo 2200 &gt; &#x2F;proc&#x2F;sys&#x2F;vm&#x2F;nr_hugepages\n\n\n将该值保存在/etc/sysctl.conf中，以便在重新启动后保留该值：\n\nsys.nr_hugepages&#x3D;2200\n\n\n在许多版本的Linux上，用户可以分配的巨大页面内存量是有限的。编辑&#x2F;etc&#x2F;security&#x2F;limits.conf文件，并为运行您的JVM的用户(例如，在本例中为用户appuser)添加成员锁条目：\n\nappuser soft    memlock        4613734400\nappuser hard    memlock        4613734400\n\n如果修改了limits.conf文件，则用户必须重新登录才能使值生效。此时，JVM应该能够分配必要的大页面。要验证它是否正常工作，请运行以下命令：\n# java -Xms4G -Xmx4G -XX:+UseLargePages -version\njava version &quot;1.8.0_201&quot;\nJava(TM) SE Runtime Environment (build 1.8.0_201-b09)\nJava HotSpot(TM) 64-Bit Server VM (build 25.201-b09, mixed mode)\n\n该命令的成功完成表明巨大的页面配置正确。如果超大页面内存配置不正确，则会给出警告：\nJava HotSpot(TM) 64-Bit Server VM warning:\nFailed to reserve shared memory (errno &#x3D; 22).\n\n程序在这种情况下运行；它只是使用常规页面，而不是大页面。\nLinux透明巨型页面从2.6.32版开始的Linux内核支持透明的巨大页面。它们(理论上)提供了与传统巨型页面相同的性能优势，但它们与传统巨型页面有一些不同。\n首先，传统的大页面被锁定在内存中；它们永远不能交换。对于Java来说，这是一个优势，因为正如我们已经讨论过的，交换堆的一部分不利于GC性能。透明的巨大页面可能会被交换到磁盘，这不利于性能。\n其次，透明巨型页面的分配也与传统的巨型页面有很大不同。传统的巨大页面在内核引导时被搁置；它们始终可用。透明的巨大页面是按需分配的：当应用程序请求2 MB的页面时，内核将尝试在物理内存中为该页面找到2 MB的连续空间。如果物理内存是碎片化的，内核可能会决定花时间在一个类似于Java堆中的内存压缩的过程中重新安排页面。这意味着分配页面的时间可能要长得多，因为它需要等待内核完成为内存腾出空间。\n这会影响所有程序，但对于Java，它可能会导致非常长的GC暂停。在GC期间，JVM可能决定扩展堆并请求新页面。如果页面分配花费几百毫秒甚至一秒，GC时间就会受到很大影响。\n第三，透明的巨型页面在操作系统和Java级别上的配置都不同。具体内容如下。\n在操作系统级别，通过更改&#x2F;sys&#x2F;core&#x2F;mm&#x2F;transparent_hugepage&#x2F;enabled的内容来配置透明巨型页面：\n# cat &#x2F;sys&#x2F;kernel&#x2F;mm&#x2F;transparent_hugepage&#x2F;enabled\nalways [madvise] never\n# echo always &gt; &#x2F;sys&#x2F;kernel&#x2F;mm&#x2F;transparent_hugepage&#x2F;enabled\n# cat &#x2F;sys&#x2F;kernel&#x2F;mm&#x2F;transparent_hugepage&#x2F;enabled\n[always] madvise never\n\n这里的三个选项如下：\n\nalways\n在可能的情况下，所有程序都会有很大的页面。\n\n\n\nmadvise\n请求巨大页面的程序会被提供给它们；其他程序则会获得常规的(4KB)页面。\n\n\n\nnever\n没有程序会得到很大的页面，即使当他们请求它们时也是如此。\n\n\n不同版本的Linux对该设置有不同的缺省值(它可能会在未来的版本中更改)。例如，Ubuntu 18.04 LTS将缺省值设置为madvise，但CentOS 7(以及基于此的Red Hat和Oracle Enterprise Linux等供应商版本)将其设置为Always。还要注意，在云计算机上，OS镜像的供应商可能已经更改了该值；我看到Ubuntu镜像也将该值设置为Always。\n如果将该值设置为Always，则不需要在Java级别进行配置：将为JVM提供巨大的页面。事实上，在该系统上运行的所有程序都将在巨大的页面中运行。\n如果将该值设置为madvise，并且您希望JVM使用大型页面，请指定UseTransparentHugePages标志(默认情况下为FALSE)。然后，当JVM分配页面并获得巨大的页面时，它将发出适当的请求。\n可以预见的是，如果将该值设置为Never，则任何Java级参数都不会允许JVM获得巨大的页面。然而，与传统的大页面不同，如果您指定了UseTransparentHugePages标志，并且系统无法提供它们，则不会给出警告。\n由于透明巨型页面在交换和分配方面的不同，通常不建议在Java中使用它们；当然，使用它们可能会导致不可预测的暂停时间峰值。另一方面，特别是在默认情况下启用它们的系统上，您将在使用它们的大部分时间内透明地看到性能优势。但是，如果您想确保在大型页面中获得最流畅的性能，最好将系统设置为仅在请求时使用透明的大型页面，并配置传统的大型页面以供JVM使用。\n总结尽管Java堆是最受关注的内存区域，但JVM的整个占用空间对其性能至关重要，尤其是与操作系统相关的性能。本章中讨论的工具允许您跟踪随时间变化的内存占用情况(最重要的是，将重点放在JVM的已提交内存而不是驻留内存上)。\nJVM使用OS内存的某些方式–尤其是大页面–也可以调优以提高性能。长时间运行的JVM几乎总是能从使用大页面中受益，尤其是在它们拥有大堆的情况下。\n","slug":"native内存最佳实践","date":"2022-05-21T01:43:03.000Z","categories_index":"","tags_index":"JVM,Java","author_index":"李志博的博客"},{"id":"56a9693faf1b310ae2faaa7452bad4b8","title":"高级文件IO","content":"通常，人们会对CPU及其性能感到紧张。虽然很重要，但在许多实际应用程序工作负载中，拖累性能的不是CPU，而是I&#x2F;O代码路径。这是完全可以理解的；回想一下，在第2章-虚拟内存中，我们展示了与RAM相比，磁盘速度慢了几个数量级。这种情况与网络I&#x2F;O类似；因此，由于大量持续的磁盘和网络I&#x2F;O，自然会出现真正的性能瓶颈。\n在本章中，读者将学习几种提高I&#x2F;O性能的方法；一般而言，这些方法包括：\n\n充分利用内核PageCache\n向内核提供有关文件使用模式的提示和建议\n使用scatter-gather(向量化)I&#x2F;O\n利用内存映射进行文件I&#x2F;O\n了解和使用复杂的DIO和AIO技术。\n了解I&#x2F;O调度程序\n用于监视、分析和控制I&#x2F;O带宽的工具&#x2F;API&#x2F;CGroup。\n\nI&#x2F;O性能建议执行I&#x2F;O时的关键点是意识到底层存储(磁盘)硬件比RAM慢得多。因此，设计策略以最大限度地减少对磁盘的访问，并从内存中进行更多工作将总是有帮助的。事实上，在库的层面上(我们已经详细讨论过缓冲)和操作系统(通过PageCache和块I&#x2F;O层中的其他功能，事实上，甚至在现代硬件中)都将执行大量工作来确保这一点。对于(系统)应用程序开发人员，接下来将提出一些需要考虑的建议。\n如果可行，在对文件执行I&#x2F;O操作时使用较大的缓冲区(用于保存读取或写入的数据)，但有多大？一个很好的经验法则是对本地缓冲区使用与文件所在文件系统的I&#x2F;O块大小相同的大小(事实上，此字段在内部记录为文件系统I&#x2F;O的块大小)。查询它很简单：对要在其中执行I&#x2F;O的文件发出stat(1)命令。例如，假设在Ubuntu 18.04系统上，我们希望读入当前运行的内核的配置文件的内容：\n$ uname -r\n4.15.0-23-generic\n$ ls -l &#x2F;boot&#x2F;config-4.15.0-23-generic \n-rw-r--r-- 1 root root 216807 May 23 22:24 &#x2F;boot&#x2F;config-4.15.0-23-generic\n$ stat &#x2F;boot&#x2F;config-4.15.0-23-generic \n File: &#x2F;boot&#x2F;config-4.15.0-23-generic\n Size: 216807 Blocks: 424 IO Block: 4096 regular file\nDevice: 801h&#x2F;2049d Inode: 398628 Links: 1\nAccess: (0644&#x2F;-rw-r--r--) Uid: ( 0&#x2F; root) Gid: ( 0&#x2F; root)\nAccess: 2018-07-30 12:42:09.789005000 +0530\nModify: 2018-05-23 22:24:55.000000000 +0530\nChange: 2018-06-17 12:36:34.259614987 +0530\n Birth: -\n\n从代码中可以看出，stat(1)揭示了内核中文件的inode数据结构的几个文件特征(或属性)，其中包括I&#x2F;O块大小。\n在内部，stat(1)实用程序发出stat(2)系统调用，该调用解析底层文件的inode并将所有详细信息提供给用户空间。因此，当以编程方式需要时，使用[f]stat(2)API。\n此外，如果内存不是一个限制，为什么不分配一个中等到非常大的缓冲区并通过它执行I&#x2F;O；这会有帮助的。确定有多大需要在目标平台上进行一些调查；让您了解一下，在早期，管道I&#x2F;O使用一页大小的内核缓冲区；在现代的Linux内核上，管道I&#x2F;O缓冲区大小默认增加到1 MB。\nPage Cache当进程(或线程)通过使用Fread(3)或fWRITE(3)库层API执行文件I&#x2F;O时，它们最终通过Read(2)和WRITE(2)系统调用发布给底层操作系统。这些系统调用让内核执行I&#x2F;O；尽管这看起来很直观，但实际上读写系统调用并不同步；也就是说，它们可能会在实际I&#x2F;O完成之前返回。(显然，对文件的写入将是这种情况；同步读取必须将读取的数据返回到用户空间内存缓冲区；在此之前，读取是阻塞的。然而，使用异步I&#x2F;O(AIO)，甚至可以进行异步读取。)\n事实上，在内核中，每个单文件I&#x2F;O操作都缓存在称为PageCache的全局内核缓存中。因此，当进程将数据写入文件时，数据缓冲区不会立即刷新到底层块设备(磁盘或闪存)，而是缓存在PageCache中。类似地，当进程从底层块设备读取数据时，数据缓冲区不会立即复制到用户空间进程内存缓冲区；不，您猜对了，它首先存储在PageCache中(进程实际上将从那里接收数据)。\n\n\n为什么内核的PageCache中的这种缓存是有帮助的？很简单：通过利用缓存的关键属性，即缓存内存区域(RAM)和缓存区域(块设备)之间的速度差异，我们可以获得极高的性能。PageCache位于RAM中，因此，当应用程序对文件数据执行读取时，(尽可能)保持所有文件I&#x2F;O的内容被缓存几乎可以保证对缓存的命中；从RAM读取要比从存储设备读取快得多。类似地，内核不是缓慢而同步地将应用程序数据缓冲区直接写入块设备，而是将写数据缓冲区缓存在PageCache中。显然，将写入的数据刷新到底层块设备以及管理PageCache内存本身的工作完全在Linux内核的工作范围内(我们在这里不讨论这些内部细节)。\n向内核提供有关文件I&#x2F;O模式的提示我们现在了解到，内核继续将所有文件I&#x2F;O缓存在其页面缓存中；这对性能有好处。思考一个例子是很有用的：应用程序设置并对非常大的视频文件执行流读取(以在某个应用程序窗口中向用户显示它；我们将假设该特定视频文件是第一次被访问)。很容易理解，一般来说，在从磁盘读取文件时缓存文件会有所帮助，但在这里，在这种特殊情况下，它不会有太大帮助，因为第一次，我们仍然必须首先到磁盘上读取它。因此，我们耸耸肩，继续以通常的方式对其进行编码，顺序读取视频数据块(通过其底层编解码器)，并将其传递给呈现代码。\n通过POSIX_fise(2)API我们能做得更好吗？的确如此：Linux提供了posix_fadvise系统调用，允许应用程序进程通过一个名为advise的参数向内核提供有关其文件数据访问模式的提示。与我们的示例相关的是，我们可以将通知作为值POSIX_FADV_SEQUENCE、POSIX_FADV_WILLNEED传递，以通知内核我们希望按顺序读取文件数据，并且我们希望在不久的将来需要访问文件数据。该建议会导致内核按顺序(从低到高的文件偏移量)对内核页面缓存启动积极的文件数据预读。这将极大地帮助提高性能。\nposix_fadvise系统调用的签名如下：\n#include &lt;fcntl.h&gt;\nint posix_fadvise(int fd, off_t offset, off_t len, int advice);\n\n显然，第一个参数fd表示文件描述符，第二个和第三个参数offset和len指定文件的一个区域，我们通过第四个参数advice在该区域上传递提示或建议。(长度实际上向上舍入为页面粒度。)\n不仅如此，在完成对视频数据块的处理后，应用程序甚至可以通过调用POSIX_FADVDONTNEED值设置为POSIX_FADV_DONTNEED值的POSIX_FADVE来向操作系统指定它将不再需要该特定的内存块；这将是对内核的一个提示，它可以释放保存该数据的页面缓存的页面，从而为传入的重要数据(以及可能仍然有用的已经缓存的数据)创造空间。\n有一些需要注意的事项。首先，对于开发人员来说，重要的是要认识到这个建议实际上只是对操作系统的一个提示和建议；它可能会得到尊重，也可能不会得到尊重。接下来，同样，即使目标文件的页面被读取到页面缓存中，它们也可能因为各种原因而被逐出，内存压力是典型的原因。不过，尝试一下也没什么坏处；内核通常会考虑这些建议，而且它确实可以提高性能。(可以像往常一样在与此API相关的手册页中查找更多建议值。)\n通过readahead API在执行积极的文件预读方面，特定于Linux(GNU)的readahead系统调用实现了与我们刚才看到的posix_fadvise类似的结果。其签名如下：\ninclude &lt;fcntl.h&gt;\nssize_t readahead(int fd, off64_t offset, size_t count);\n\n在fd指定的目标文件上执行预读，从文件偏移量开始，最大计数字节数(四舍五入到页面粒度)。\n使用pread 、pwrite API的MT应用程序文件I&#x2F;O回想一下read和write系统调用；它们构成了对文件执行I&#x2F;O的基础。您还会记得，在使用这些API时，操作系统将隐式更新底层文件偏移量。例如，如果进程打开一个文件(通过open)，然后执行512字节的read，则文件的偏移量(或所谓的查找位置)现在将是512。如果它现在写入比方说200字节，则写入将从位置512发生到位置712，从而将新的寻道位置或偏移量设置为该数字。\n好吧，那又怎样？我们的观点很简单，当多线程应用程序有多个线程同时在同一底层文件上执行I&#x2F;O时，隐式设置文件的偏移量会导致问题。但请稍等，我们之前已经提到过这一点：需要锁定文件，然后再对其进行操作。但是，锁定会造成主要的性能瓶颈。如果你设计了一个MT应用程序，它的线程并行地处理同一文件的不同部分，会怎么样？这听起来很棒，只是文件的偏移量会不断变化，从而破坏我们的并行性，从而破坏性能。\n那么，你是做什么的？Linux为此提供了pread和pwrite系统调用(p表示定位的I&#x2F;O)；使用这些API，可以指定(或定位)执行I&#x2F;O的文件偏移量，并且操作系统不会更改实际的底层文件偏移量。他们的签名如下：\n#include &lt;unistd.h&gt;\nssize_t pread(int fd, void *buf, size_t count, off_t offset);\nssize_t pwrite(int fd, const void *buf, size_t count, off_t offset);\n\npread&#x2F;pwrite和通常的read&#x2F;write系统调用之间的区别在于，前面的API采用额外的第四个参数-执行读或写I&#x2F;O操作的文件偏移量，而不修改它。\n这使我们能够实现我们想要的：通过让多个线程同时并行读写文件的不同部分，让MT应用程序执行高性能I&#x2F;O。\n需要注意的几点：首先，与read和write一样，pread和pwrite也可以在没有传输所有请求的字节的情况下返回；程序员有责任在循环中检查和调用API，直到没有剩余的字节要传输。正确使用读&#x2F;写API，其中解决了此类问题。其次，当使用指定的O_APPEND标志打开文件时，Linux的pwrite系统调用总是将数据附加到EOF，而不考虑当前的偏移量；这违反了POSIX标准，该标准规定O_APPEND标志不应对发生写入的起始位置产生影响。第三，非常明显(但我们必须声明)，被操作的文件必须能够被搜索(即，支持fseek或lseekAPI)。常规文件始终支持查找操作，但管道和某些类型的设备不支持。\nScatter （分散）– gather（聚集） I&#x2F;O为了帮助解释这个主题，假设我们被委托将数据写入文件，从而写入三个不连续的数据区域A、B和C(分别用AS、BS和Cs填充)；下图显示了这一点：\n\n\n注意文件是如何有洞的–不包含任何数据内容的区域；这可以使用常规文件(主要是空洞的文件称为稀疏文件)来实现。你是如何创建这个洞的？很简单：只需执行lseek，然后write数据；向前搜索的长度决定了文件中漏洞的大小。那么，我们如何实现所示的数据文件布局呢？我们将展示两种方法–一种是传统方式，另一种是更优化的性能方法。让我们从传统的方法开始。\n非连续数据文件-传统方法这似乎很简单：首先查找到所需的起始偏移量，然后写入所需长度的数据内容；这可以通过一对lseek和write系统调用完成。当然，我们必须调用这对系统调用三次。因此，我们编写了一些代码来实际执行此任务；请参见此处的代码(相关片段)(ch18&#x2F;sgio_imple.c)：\n#define A_HOLE_LEN  10\n#define A_START_OFF A_HOLE_LEN\n#define A_LEN       20\n\n#define B_HOLE_LEN  100\n#define B_START_OFF (A_HOLE_LEN+A_LEN+B_HOLE_LEN)\n#define B_LEN        30\n\n#define C_HOLE_LEN  20\n#define C_START_OFF (A_HOLE_LEN+A_LEN+B_HOLE_LEN+B_LEN+C_HOLE_LEN)\n#define C_LEN       42\n...\nstatic int wr_discontig_the_normal_way(int fd)\n&#123; ...\n    &#x2F;* A: &#123;seek_to A_START_OFF, write gbufA for A_LEN bytes&#125; *&#x2F;\n    if (lseek(fd, A_START_OFF, SEEK_SET) &lt; 0)\n        FATAL(&quot;lseek A failed\\n&quot;);\n    if (write(fd, gbufA, A_LEN) &lt; 0)\n        FATAL(&quot;write A failed\\n&quot;);\n\n    &#x2F;* B: &#123;seek_to B_START_OFF, write gbufB for B_LEN bytes&#125; *&#x2F;\n    if (lseek(fd, B_START_OFF, SEEK_SET) &lt; 0)\n        FATAL(&quot;lseek B failed\\n&quot;);\n    if (write(fd, gbufB, B_LEN) &lt; 0)\n        FATAL(&quot;write B failed\\n&quot;);\n\n    &#x2F;* C: &#123;seek_to C_START_OFF, write gbufC for C_LEN bytes&#125; *&#x2F;\n    if (lseek(fd, C_START_OFF, SEEK_SET) &lt; 0)\n        FATAL(&quot;lseek C failed\\n&quot;);\n    if (write(fd, gbufC, C_LEN) &lt; 0)\n        FATAL(&quot;write C failed\\n&quot;);\n    return 0;\n&#125;\n\n请注意，我们如何连续三次编写代码来使用一对系统调用&#123;lseek, write&#125;，让我们来尝试一下：\n$ .&#x2F;sgio_simple \nUsage: .&#x2F;sgio_simple use-method-option\n 0 &#x3D; traditional lseek&#x2F;write method\n 1 &#x3D; better SG IO method\n$ .&#x2F;sgio_simple 0\nIn setup_buffers_goto()\nIn wr_discontig_the_normal_way()\n$ ls -l tmptest \n-rw-rw-r--. 1 kai kai 222 Oct 16 08:45 tmptest\n$ hexdump -x tmptest \n0000000 0000 0000 0000 0000 0000 4141 4141 4141\n0000010 4141 4141 4141 4141 4141 4141 4141 0000\n0000020 0000 0000 0000 0000 0000 0000 0000 0000\n*\n0000080 0000 4242 4242 4242 4242 4242 4242 4242\n0000090 4242 4242 4242 4242 4242 4242 4242 4242\n00000a0 0000 0000 0000 0000 0000 0000 0000 0000\n00000b0 0000 0000 4343 4343 4343 4343 4343 4343\n00000c0 4343 4343 4343 4343 4343 4343 4343 4343\n00000d0 4343 4343 4343 4343 4343 4343 4343 \n00000de\n\n它起作用了；我们创建的文件tmptest的长度为222字节，尽管实际数据内容(AS、BS和Cs)的长度为20+30+42&#x3D;92字节。剩下的(222-92)130个字节是文件中的三个洞。hexdump实用程序可以方便地转储文件的内容；0x41表示A，0x42表示B，0x43表示C。可以清楚地看到空洞是我们想要的长度的空填充区域。\n不连续数据文件-SG-I&#x2F;O方法使用连续三次的系统调用对的传统方法当然有效，但性能损失相当大；事实上，发出系统调用被认为是非常昂贵的。在性能方面，一种更优越的方法称为分散聚集I&#x2F;O(SG-I&#x2F;O，或向量化I&#x2F;O)。相关的系统调用是readv和writev；这是它们的签名：\n#include &lt;sys&#x2F;uio.h&gt;\nssize_t readv(int fd, const struct iovec *iov, int iovcnt);\nssize_t writev(int fd, const struct iovec *iov, int iovcnt);\n\n这些系统调用允许您指定一组段以一次读取或写入；每个段通过名为iovec的结构描述单个I&#x2F;O操作：\nstruct iovec &#123;\n    void *iov_base; &#x2F;* Starting address *&#x2F;\n    size_t iov_len; &#x2F;* Number of bytes to transfer *&#x2F;\n&#125;;\n\n程序员可以传递描述要执行的I&#x2F;O操作的段数组；这正是第二个参数-指向struct iovecs数组的指针；第三个参数是要处理的段数。第一个参数很明显–文件描述符，表示要对其执行集中读取或分散写入的文件。\n因此，想想看：您可以将来自给定文件的不连续读取聚集到您通过I&#x2F;O向量指针指定的缓冲区(及其大小)中，并且可以从您通过I&#x2F;O向量指针指定的缓冲区(及其大小)分散对给定文件的不连续写入；因此，这些类型的多个不连续I&#x2F;O操作被称为scatter-gatherI&#x2F;O！这是真正酷的部分：系统调用保证以数组顺序和原子方式执行这些I&#x2F;O操作；也就是说，只有当所有操作完成时，它们才会返回。不过，还是要小心：从readv或writev返回的值是实际读取或写入的字节数，如果失败，返回值为-1。I&#x2F;O操作执行的数量总是可能少于请求的数量；这不是故障，应该由开发人员进行检查。\n现在，对于前面的数据文件示例，让我们看一下通过writev设置和执行不连续的分散有序原子写入的代码：\nstatic int wr_discontig_the_better_SGIO_way(int fd)\n&#123;\n  struct iovec iov[6];\n  int i&#x3D;0;\n\n  &#x2F;* We don&#39;t want to call lseek of course; so we emulate the seek\n   * by introducing segments that are just &quot;holes&quot; in the file. *&#x2F;\n\n  &#x2F;* A: &#123;seek_to A_START_OFF, write gbufA for A_LEN bytes&#125; *&#x2F;\n  iov[i].iov_base &#x3D; gbuf_hole;\n  iov[i].iov_len &#x3D; A_HOLE_LEN;\n  i ++;\n  iov[i].iov_base &#x3D; gbufA;\n  iov[i].iov_len &#x3D; A_LEN;\n\n  &#x2F;* B: &#123;seek_to B_START_OFF, write gbufB for B_LEN bytes&#125; *&#x2F;\n  i ++;\n  iov[i].iov_base &#x3D; gbuf_hole;\n  iov[i].iov_len &#x3D; B_HOLE_LEN;\n  i ++;\n  iov[i].iov_base &#x3D; gbufB;\n  iov[i].iov_len &#x3D; B_LEN;\n\n  &#x2F;* C: &#123;seek_to C_START_OFF, write gbufC for C_LEN bytes&#125; *&#x2F;\n  i ++;\n  iov[i].iov_base &#x3D; gbuf_hole;\n  iov[i].iov_len &#x3D; C_HOLE_LEN;\n  i ++;\n  iov[i].iov_base &#x3D; gbufC;\n  iov[i].iov_len &#x3D; C_LEN;\n  i ++;\n\n  &#x2F;* Perform all six discontiguous writes in order and atomically! *&#x2F;\n  if (writev(fd, iov, i) &lt; 0)\n    return -1;\n&#x2F;* Do note! As mentioned in Ch 19:\n   * &quot;the return value from readv(2) or writev(2) is the actual number\n   * of bytes read or written, and -1 on failure. It&#39;s always possible\n   * that an I&#x2F;O operation performs less than the amount requested; this\n   * is not a failure, and it&#39;s up to the developer to check.&quot;\n   * Above, we have _not_ checked; we leave it as an exercise to the\n   * interested reader to modify this code to check for and read&#x2F;write\n   * any remaining bytes (similar to this example: ch7&#x2F;simpcp2.c).\n   *&#x2F;\n  return 0;\n&#125;\n\n最终结果与传统方法相同；我们将其留给读者来尝试和查看。这是关键点：传统方法要求我们至少发出六个系统调用(3个&#123;lSeek，WRITE&#125;对)来执行不连续数据写入文件，而SG-I&#x2F;O代码只使用一个系统调用执行完全相同的不连续数据写入。这会带来显著的性能提升，尤其是对于I&#x2F;O工作负载繁重的应用程序。\nSG-I&#x2F;O变化回想一下在MT应用程序文件I&#x2F;O中使用pread，pWRITE API一节，我们可以使用prew(2)和pwrite(2)系统调用通过多线程(在多线程应用程序中)有效地并行执行文件I&#x2F;O。类似地，Linux提供了preadv和pwritev系统调用；正如您可以猜到的那样，它们通过添加第四个参数偏移量提供了readv和writev的功能；就像使用readv和writev一样，可以指定要执行SG-IO的文件偏移量，并且不会更改它(同样，对于MT应用程序可能很有用)。preadv和pwritev的签名如下所示：\n#include &lt;sys&#x2F;uio.h&gt;\nssize_t preadv(int fd, const struct iovec *iov, int iovcnt,\n                      off_t offset);\nssize_t pwritev(int fd, const struct iovec *iov, int iovcnt,\n                       off_t offset);\n\n最新的Linux内核(有些是4.6版以上)还提供了API的进一步变体：preadv2和pwritev2系统调用。与以前的API的不同之处在于，它们采用额外的第五个参数标志，允许开发人员通过能够指定它们是同步(通过RWF_DSYNC和RWF_SYNC标志)、高优先级(通过RWF_HIPRI标志)还是非阻塞(通过RWF_NOWIT标志)来更好地控制SG-I&#x2F;O操作的行为。有关详细信息，请读者参阅preadv2&#x2F;pwritev上的手册页。\n通过内存映射的I&#x2F;O文件在本章中，我们都多次提到Linux内核的PageCache如何通过缓存其中的文件内容来帮助极大地提高性能(减少了每次访问非常慢的存储设备而只读或写RAM中的数据块的需要)。然而，尽管我们通过页面缓存获得了性能，但同时使用传统的read write api 和甚至更快的pread   pwrite readv writev preadv pwritev api,但是仍然存在1个隐藏的问题。\nLinux I&#x2F;O代码路径简介要了解问题所在，我们首先必须更深入地了解I&#x2F;O代码路径的实际工作方式；下图概括了相关要点：\n\n\n\n\n假设进程P1打算从它已打开的目标文件中读取大约12KB的数据(通过open系统调用)；我们设想它将通过通常的方式完成此操作：\n\n通过mallocAPI分配12KB的堆缓冲区(3页&#x3D;12,288字节)。\n发出read系统调用，将数据从文件读入堆缓冲区。\nread系统调用在操作系统中执行工作；当读取完成时，它返回(希望返回值12,288；记住，程序员的工作是检查这一点，并且不做任何假设)。\n\n\n\n这听起来很简单，但在幕后还有更多的事情发生，深入挖掘一下符合我们的利益。下面是所发生情况的更详细的视图(在上图中，数字点1、2和3在一个圆圈中显示；下面是)：\n\n进程P1通过mallocAPI(len&#x3D;12KB&#x3D;12,288字节)分配12KB的堆缓冲区。\n接下来，它发出一个read系统调用，将数据从文件(由fd指定)读取到刚刚分配的堆缓冲区buf中，长度为12KB。\n因为read是一个系统调用，所以进程(或线程)现在切换到内核模式(还记得我们在第1章，Linux系统体系结构中介绍的单片设计吗)；它进入Linux内核的通用文件系统层(称为虚拟文件系统交换机(VFS))，在那里它将被自动分流到其适当的底层文件系统驱动程序(可能是ext4文件系统驱动程序)上，之后Linux内核将首先检查：所需文件数据的这些页面是否已缓存在我们的页面缓存中？如果是，则作业已完成(我们跳到步骤7)，只需将页面复制回用户空间缓冲区。假设我们得到一个缓存未命中–所需的文件数据页不在页面缓存中。\n因此，内核首先为页面缓存分配足够的RAM(页帧)(在我们的示例中，页面缓存内存区域中显示为粉红色正方形的三个帧)。然后，它向请求文件数据的底层发出适当的I&#x2F;O请求。\n请求最终到达块(存储)驱动程序；我们假设它知道自己的工作，并从底层存储设备控制器(可能是磁盘或闪存控制器芯片)读取所需的数据块。然后(有趣的是)它被赋予一个目标地址来写入文件数据；它是页面缓存中分配的页帧的地址(步骤4)；因此，块驱动程序总是将文件数据写入内核的页面缓存，而不是直接返回到用户模式进程缓冲区。\n块驱动程序已成功地将数据块从存储设备(或其他设备)复制到内核页面缓存中先前分配的帧中。(实际上，这些数据传输通过一种称为直接内存访问(DMA)的高级内存传输技术进行了高度优化，在这种技术中，驱动程序本质上利用硬件在设备和系统内存之间直接传输数据，而无需CPU干预。显然，这些主题远远超出了本书的范围。)\n现在，内核将刚刚填充的内核页面缓存帧复制到用户空间堆缓冲区中。\n(阻塞的)read系统调用现在终止，返回值12,288，表示所有三页文件数据确实已被传输(同样，作为应用程序开发人员，您应该检查该返回值并不做任何假设)。\n\n一切看起来都很棒，对吧？其实并非如此；仔细考虑一下这一点：尽管readAPI确实成功了，但这一成功是付出了相当大的代价的：内核必须分配RAM(页帧)以在其页缓存中保存文件数据(步骤4)，一旦数据传输完成(步骤6)，然后将内容复制到用户空间堆内存(步骤7)。因此，通过保留额外的数据副本，我们使用了两倍于应有的内存量。这是非常浪费的，显然，数据缓冲区在块驱动程序到内核页面缓存，然后内核页面缓存到用户空间堆缓冲区之间的多次复制也会降低性能(更不用说CPU缓存会不必要地处理所有这些垃圾内容)。使用前面的代码模式，不等待速度较慢的存储设备的问题得到了解决(通过页面缓存效率)，但其他方面都很差-我们实际上已经将所需的内存使用量增加了一倍，并且在进行复制时，CPU缓存被(不必要的)文件数据覆盖。\n为I&#x2F;O映射文件的内存以下是这些问题的解决方案：通过mmap系统调用进行内存映射。Linux提供了非常强大的mmap系统调用；它使开发人员能够将任何内容直接映射到进程虚拟地址空间(VAS)。该内容包括文件数据、硬件设备(适配器)存储区域或仅通用存储区域。在本章中，我们将只关注使用mmap将常规文件的内容映射到进程VAS中。在讨论mmap如何成为我们刚才讨论的内存浪费问题的解决方案之前，我们首先需要更多地了解如何使用mmap系统调用本身。\nmmap系统调用的签名如下所示：\n#include &lt;sys&#x2F;mman.h&gt;\nvoid *mmap(void *addr, size_t length, int prot, int flags,\n           int fd, off_t offset);\n\n我们希望将文件的给定区域从给定的offset和length字节映射到我们的Process VAS中；下图描述了我们想要实现的简单视图：\n\n为了实现到进程VAS的文件映射，我们使用mmap系统调用。看一眼它的签名，很明显我们首先需要做的是：通过open打开要映射的文件(以适当的模式：只读或读写，取决于您想要做什么)，从而获得文件描述符；将该描述符作为第五个参数传递给mmap。要映射到进程VAS的文件区域可以分别通过第六个和第二个参数指定–映射应开始的文件偏移量和长度(以字节为单位)。\n第一个参数addr提示内核应该在进程VAS中的什么位置创建映射；建议在这里传递0(空)，允许操作系统决定新映射的位置。这是使用mmap的正确的便携方式；但是，有些应用程序(当然，还有一些恶意的安全黑客！)使用此参数可以尝试预测发生映射的位置。在任何情况下，在流程VAS中创建映射的实际(虚拟)地址是来自mmap的返回值；空返回值表示失败，必须进行检查。\n这里有一个确定映射位置的有趣技术：首先执行一个所需映射大小的malloc，并将返回值从这个malloc传递给mmap的第一个参数(还设置标志参数以包括MAP_FIXED位)！如果长度大于MMAP_THRESHOLD(默认情况下为128KB)并且大小是系统页面大小的倍数，则此方法可能会起作用。请再次注意，此技术不可移植，可能会也可能不会起作用。\n另一点需要注意的是，大多数映射(总是文件映射)都是按页面粒度执行的，即以页面大小的倍数执行；因此，返回地址通常是页面对齐的。\nmmap的第三个参数是一个整数位掩码prot–给定区域的内存保护(回想一下，我们已经在内存保护一节的第4章动态内存分配中遇到过内存保护)。Prot参数是位掩码，可以只是PROT_NONE位(表示没有权限)，也可以是余数的按位或；此表列举了位及其含义：\n\n\n\n保护位\n含义\n\n\n\nPROT_NONE\n不允许对页面进行访问\n\n\nPROT_READ\n页面上允许的读取数\n\n\nPROT_WRITE\n页面上允许的写\n\n\nPROT_EXEC\n执行页面上允许的访问权限\n\n\n当然，页面保护必须与文件的open相匹配。还要注意，在较旧的x86系统上，可写内存用于表示可读内存(即PROT_WRITE&#x3D;&gt;PROT_READ)。现在不再是这种情况了；您必须显式指定映射的页面是否可读(对于可执行页面也是如此：必须指定，文本段就是典型的例子)。为什么要使用PROT_NONE？保护页面就是一个实际的例子。\nmmap的优势现在我们已经了解了如何使用mmap系统调用，我们再回顾一下前面的讨论：回想一下，使用read/write会导致双重复制；内存浪费(加上CPU缓存也被丢弃的事实)。\n实现mmap如此有效地解决这个严重问题的关键在于：mmap通过在内部将包含文件数据(从存储设备读取)的内核页缓存页直接映射到进程虚拟地址空间来建立文件映射。此图说明了这一点：\n\n\n\n\n映射不是副本；因此，基于mmap的文件I&#x2F;O被称为零复制技术：一种在I&#x2F;O缓冲区上执行工作的方法，其中只有一个副本由内核在其页缓存中维护；不需要更多的副本。\n事实是，设备驱动程序的作者希望使用零复制技术来优化他们的数据路径，mmap肯定是其中之一。\nmmap在设置映射(第一次)时确实会产生很大的开销，但一旦完成，I&#x2F;O就会非常快，因为它基本上是在内存中执行的。将mmap用于非常少量的I&#x2F;O工作可能并不是最优的；当指示大而持续的I&#x2F;O工作负载时，建议使用它。\n","slug":"高级文件IO","date":"2022-05-15T14:50:22.000Z","categories_index":"","tags_index":"linux,IO","author_index":"李志博的博客"},{"id":"7a8ccfb903ca33a7195e4b4486061453","title":"RocketMQ-NameServer的源码阅读脉络","content":"启动大概流程\nNamesrvStartup 是启动类，会先调createNamesrvController 初始化配置，然后创建NamesrvController, 然后调Controller的静态start方法\nNamesrvController的静态Start方法会先调initialize方法，然后注册程序destory的钩子，最后start方法启动\n在initialize 方法里会执行registerProcessor方法，会在这里注册一个NettyRequestProcessor 这个是1个接口，表示的是RocketMQ请求处理，在NameServer里，有2个实现，ClusterTestRequestProcessor 和 DefaultRequestProcessor其中我们主要关键的是DefaultRequestProcessor 。\n\n\n\n处理请求路由DefaultRequestProcessor是NameServer里我觉得最核心的类，因为它是NameServer里负责接收并处理请求的类，通过它的processRequest方法，可以很直接的看到NameServer都具体负责处理哪些请求，以及这些请求具体的实现，也都可以找的到。\n核心类介绍\nKVConfigManager 负责管理不同Namespace下KV的配置，并存储。\nRouteInfoManager 负责管理路由相关信息\nNamesrvController 是1个集中的初始化整个系统的类，并且持有其他核心类的依赖，其他类核心类都持有NamesrvController的引用，要跟另外个类调用，都是通过NamesrvController来获取这个类的引用，相当于1个解耦的作用。\n\n","slug":"RocketMQ-NameServer的源码阅读脉络","date":"2022-05-15T04:00:02.000Z","categories_index":"","tags_index":"RocketMQ","author_index":"李志博的博客"},{"id":"957c5556bfb48cc069f3d5abcb598019","title":"Maven多模块统一管理版本","content":"之前写多模块项目的时候，一般都需要保证各模块的版本号是一致的，所以导致每次代码有变动需要升级版本的时候，都需要全局挨个替换，但是这种方式太麻烦，也不够优雅，由于最近工作是维护公司的apollo，所以我看了下apollo的pom文件里的方式，觉得还不错。\n首先是要在多模块项目里的父pom上，做如下配置\n&lt;groupId&gt;com.example&lt;&#x2F;groupId&gt;\n&lt;artifactId&gt;demo-mulitmodule-release-plugin&lt;&#x2F;artifactId&gt;\n&lt;version&gt;$&#123;reversion&#125;&lt;&#x2F;version&gt;\n&lt;packaging&gt;pom&lt;&#x2F;packaging&gt;\n&lt;properties&gt;\n\t&lt;reversion&gt;0.0.1-SNAPSHOT&lt;&#x2F;reversion&gt;\n&lt;&#x2F;properties&gt;\n\n为了精简，以上我只贴了相关的配置，packaging 是写死的，必须是pom,version要通过properties设置属性引用的方式，这样方便在后续子pom里通过同样的方式去引用，这样后续版本变化只改这里就可以了。groupId和artifactId 写自己的就行，这里只是为了合后面子pom的配置保持上下文一致。\n然后子pom的相关配置如下:\n&lt;parent&gt;\n\t&lt;groupId&gt;com.example&lt;&#x2F;groupId&gt;\n\t&lt;artifactId&gt;demo-mulitmodule-release-plugin&lt;&#x2F;artifactId&gt;\n\t&lt;version&gt;$&#123;reversion&#125;&lt;&#x2F;version&gt;\n\t&lt;relativePath&gt;..&#x2F;pom.xml&lt;&#x2F;relativePath&gt;\n&lt;&#x2F;parent&gt;\n\n上面配置就是明确子pom继承了前面的父pom，并通过$&#123;reversion&#125;实现父pom版本号的引用和relativePath的设置来定位父pom文件的位置。\n","slug":"Maven多模块统一管理版本","date":"2022-05-14T05:27:00.000Z","categories_index":"","tags_index":"maven","author_index":"李志博的博客"},{"id":"472d9f1a708133ccb720a7af7f9b0d65","title":"MySQL查询Value区分大小写","content":"今天1个同事问我1个问题，MySQL查询如何区分大小写，当时我就惊了，难道默认不区分吗？于是我做了一下尝试，在本地建了1个表，发现还真的不区分，效果如下: \n\n\n然后我执行下面的SQL发现居然可以把zhangsan 查的到\nselect * from student where name &#x3D; &#39;Zhangsan&#39;\n\n其中Z是我故意大写的，结果如下:\n\n\n要想解决这个问题，可以通过binary关键字解决，加在列的前面\nselect * from student where binary name &#x3D; &#39;Zhangsan&#39;\n\n这样就查不到了\n\n\n\n\n","slug":"MySQL查询Value区分大小写","date":"2022-05-14T02:23:27.000Z","categories_index":"","tags_index":"mysql","author_index":"李志博的博客"}]